{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d545b75d",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "### Import necessary modules and do some basic setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1427bd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= '0.20'\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "assert tf.__version__ >= '2.0'\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# Common imports\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import dask\n",
    "import datetime\n",
    "import math\n",
    "dask.config.set({'array.slicing.split_large_chunks': False})\n",
    "\n",
    "# To make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# Config matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Dotenv\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "# Custom utils\n",
    "from utils.utils_data import *\n",
    "from utils.utils_ml import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41766d4a",
   "metadata": {},
   "source": [
    "### Define some paths and constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc1d02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dotenv_values(\".env\")\n",
    "\n",
    "# Paths\n",
    "PATH_ERA5 = config['PATH_ERA5']\n",
    "PATH_EOBS = config['PATH_EOBS']\n",
    "\n",
    "# Some constants\n",
    "G = 9.80665 \n",
    "DATE_START = '1979-01-01'\n",
    "DATE_END = '2020-12-31'\n",
    "YY_TRAIN = [1979, 2015]\n",
    "YY_TEST = [2016, 2020]\n",
    "LEVELS = [500, 850, 1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c3dbae",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74596028",
   "metadata": {},
   "source": [
    "## Target variable: precipitation field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abeea2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precipitation ERA5\n",
    "pr = get_era5_data(PATH_ERA5 + '/precipitation/day_grid1/*nc', DATE_START, DATE_END)\n",
    "\n",
    "# Define precipitation extremes using the 95th percentile\n",
    "pr95 = precip_exceedance_xarray(pr, 0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c847649a",
   "metadata": {},
   "source": [
    "## Input data: meteorological fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce5176d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load geopotential height\n",
    "z = get_era5_data(PATH_ERA5 + '/geopotential/grid1/*.nc', DATE_START, DATE_END)\n",
    "z = z.sel(level=LEVELS)\n",
    "\n",
    "# Get Z in geopotential height (m)\n",
    "z.z.values = z.z.values/G\n",
    "\n",
    "# Get axes\n",
    "lats = z.lat\n",
    "lons = z.lon\n",
    "\n",
    "# Load temperature\n",
    "t2m = get_era5_data(PATH_ERA5 + '/temperature/grid1/Grid1_Daymean_era5_T2M_EU_19790101-20211231.nc',\n",
    "                    DATE_START, DATE_END)\n",
    "t2m['time'] = pd.DatetimeIndex(t2m.time.dt.date)\n",
    "t2m = t2m.rename_vars({'T2MMEAN': 't'})\n",
    "\n",
    "# Load relative humidity\n",
    "rh = get_era5_data(PATH_ERA5 + '/relative_humidity/day_grid1/*.nc',\n",
    "                   DATE_START, DATE_END)\n",
    "rh['time'] = pd.DatetimeIndex(rh.time.dt.date)\n",
    "rh = rh.sel(level=LEVELS)\n",
    "\n",
    "# Load wind components\n",
    "u850 = get_era5_data(PATH_ERA5 + '/U_wind/day_grid1/*.nc',\n",
    "                     DATE_START, DATE_END)\n",
    "u850['time'] = pd.DatetimeIndex(u850.time.dt.date)\n",
    "v850 = get_era5_data(PATH_ERA5 + '/V_wind/day_grid1/*.nc',\n",
    "                     DATE_START, DATE_END)\n",
    "v850['time'] = pd.DatetimeIndex(v850.time.dt.date)\n",
    "\n",
    "# Checking dimensions\n",
    "print('dimension of z', z.dims)\n",
    "print('dimension of t2m:', t2m.dims)\n",
    "print('dimension of rh:', rh.dims)\n",
    "print('dimension of u:', u850.dims)\n",
    "print('dimension of v:', v850.dims)\n",
    "print('dimension of pr:', pr.dims)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d54b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge arrays\n",
    "X = xr.merge([z, t2m, rh, u850, v850])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7643ef",
   "metadata": {},
   "source": [
    "### Split data and transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23738ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and test\n",
    "X_train_full = X.sel(time=slice('{}-01-01'.format(YY_TRAIN[0]),\n",
    "                                '{}-12-31'.format(YY_TRAIN[1])))\n",
    "X_test = X.sel(time=slice('{}-01-01'.format(YY_TEST[0]),\n",
    "                          '{}-12-31'.format(YY_TEST[1])))\n",
    "\n",
    "pr_train_full = pr.sel(time=slice('{}-01-01'.format(YY_TRAIN[0]),\n",
    "                                  '{}-12-31'.format(YY_TRAIN[1])))\n",
    "pr_test = pr.sel(time=slice('{}-01-01'.format(YY_TEST[0]),\n",
    "                            '{}-12-31'.format(YY_TEST[1])))\n",
    "xtr_train_full = pr95.sel(time=slice('{}-01-01'.format(YY_TRAIN[0]),\n",
    "                                     '{}-12-31'.format(YY_TRAIN[1])))\n",
    "xtr_test = pr95.sel(time=slice('{}-01-01'.format(YY_TEST[0]),\n",
    "                               '{}-12-31'.format(YY_TEST[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be5e320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data generator\n",
    "dic = {'z': LEVELS,\n",
    "       't': None,\n",
    "       'r': LEVELS,\n",
    "       'u': None,\n",
    "       'v': None}\n",
    "\n",
    "data_gen_train = DataGeneratorForPrecip(X_train_full.sel(time=slice('1979', '2010')),\n",
    "                                        pr_train_full.sel(time=slice('1979', '2010')),\n",
    "                                        dic, batch_size=32, load=True)\n",
    "data_gen_valid = DataGeneratorForPrecip(X_train_full.sel(time=slice('2011', '2015')),\n",
    "                                        pr_train_full.sel(time=slice('2011', '2015')),\n",
    "                                        dic, mean=data_gen_train.mean, std=data_gen_train.std,\n",
    "                                        batch_size=32, load=True)\n",
    "data_gen_test = DataGeneratorForPrecip(X_test, pr_test, dic,\n",
    "                                       mean=data_gen_train.mean, std=data_gen_train.std,\n",
    "                                       batch_size=32, load=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ba7bd3",
   "metadata": {},
   "source": [
    "# Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4bb749",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EDM(tf.keras.Model):\n",
    "    \"\"\"Encoder decoder model.\"\"\"\n",
    "\n",
    "    def __init__(self, arch, input_size, output_size, for_extremes=False, latent_dim=128,\n",
    "                 dropout_rate=0.2):\n",
    "        super(EDM, self).__init__()\n",
    "        self.arch = arch\n",
    "        self.input_size = list(input_size)\n",
    "        self.output_size = list(output_size)\n",
    "        self.for_extremes = for_extremes\n",
    "        self.latent_dim = latent_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        if arch == 'cnn-v1':\n",
    "            self.create_cnnv1()\n",
    "        elif arch == 'cnn-v2':\n",
    "            self.create_cnnv2()\n",
    "        elif arch == 'cnn-v3':\n",
    "            self.create_cnnv3()\n",
    "        else:\n",
    "            raise('The architecture was not correctly defined')\n",
    "            self.create_cnnv1()\n",
    "            \n",
    "        self.crop_output()\n",
    "        \n",
    "        \n",
    "    def create_cnnv1(self):\n",
    "        self.encoder = tf.keras.Sequential(\n",
    "            [\n",
    "                layers.InputLayer(input_shape=self.input_size),\n",
    "                layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.MaxPooling2D(pool_size=2),\n",
    "                layers.SpatialDropout2D(self.dropout_rate),\n",
    "                layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.MaxPooling2D(pool_size=2),\n",
    "                layers.SpatialDropout2D(self.dropout_rate),\n",
    "                layers.Flatten(),\n",
    "                layers.Dense(self.latent_dim, activation='sigmoid'),\n",
    "                layers.Dropout(self.dropout_rate),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        preflat_shape = self.encoder.layers[-3].input.get_shape().as_list()[1:]\n",
    "\n",
    "        print(preflat_shape)\n",
    "\n",
    "        self.decoder = tf.keras.Sequential(\n",
    "            [\n",
    "                layers.InputLayer(input_shape=(self.latent_dim,)),\n",
    "                layers.Dense(np.prod(preflat_shape), activation='relu'),\n",
    "\n",
    "\n",
    "                layers.Reshape(target_shape=preflat_shape),\n",
    "                layers.Conv2DTranspose(64, 3, strides=2, padding='same', activation='relu'),\n",
    "                layers.Conv2DTranspose(32, 3, strides=2, padding='same', activation='relu'),\n",
    "                layers.Conv2DTranspose(1, 3, strides=1, padding='same', activation='relu'),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        \n",
    "    def create_cnnv2(self):\n",
    "        \"\"\" self.encoder = tf.keras.Sequential(\n",
    "            [\n",
    "                layers.InputLayer(input_shape=self.data_size),\n",
    "                layers.Conv2D(8, 3, padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "                layers.Conv2D(8, 3, strides=(2,2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "                layers.Conv2D(16, 3, strides=(2,2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "                layers.Conv2D(16, 3, strides=(2,2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "                layers.Conv2D(32, 3, strides=(2,2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "                layers.Flatten(),\n",
    "                layers.Dense(self.latent_dim, activation='sigmoid', kernel_initializer='he_normal')\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        preflat_shape = self.encoder.layers[-2].input.get_shape().as_list()[1:]\n",
    "\n",
    "        self.decoder = tf.keras.Sequential(\n",
    "            [\n",
    "                layers.InputLayer(input_shape=(self.latent_dim,)),\n",
    "                layers.Dense(np.prod(preflat_shape), activation='relu', kernel_initializer='he_normal'),\n",
    "                layers.Reshape(target_shape=preflat_shape),\n",
    "                layers.Conv2DTranspose(16, 3, strides=(2,2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "                layers.Conv2DTranspose(16, 3, strides=(2,2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "                layers.Conv2DTranspose(16, 3, strides=(2,2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "                layers.Conv2DTranspose(8, 3, strides=(2,2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "                layers.Conv2D(1, 3, strides=(1,1), padding='same', activation='relu', kernel_initializer='he_normal')\n",
    "            ]\n",
    "        ) \"\"\"\n",
    "        \n",
    "        \n",
    "    def create_cnnv3(self):\n",
    "        \"\"\" self.encoder = tf.keras.Sequential(\n",
    "            [\n",
    "                layers.InputLayer(input_shape=self.data_size),\n",
    "                layers.Conv2D(8, 3, padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "                layers.Conv2D(8, 3, strides=(2,2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "                layers.Conv2D(16, 3, strides=(2,2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "                layers.Conv2D(16, 3, strides=(2,2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "                layers.Conv2D(32, 3, strides=(2,2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "                layers.Conv2D(32, 3, strides=(2,2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "                layers.Conv2D(64, 3, strides=(2,2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "                layers.Flatten(),\n",
    "                layers.Dense(self.latent_dim, activation='sigmoid', kernel_initializer='he_normal')\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        preflat_shape = self.encoder.layers[-2].input.get_shape().as_list()[1:]\n",
    "\n",
    "        self.decoder = tf.keras.Sequential(\n",
    "            [\n",
    "                layers.InputLayer(input_shape=(self.latent_dim,)),\n",
    "                layers.Dense(np.prod(preflat_shape), activation='relu', kernel_initializer='he_normal'),\n",
    "                layers.Reshape(target_shape=preflat_shape),\n",
    "                layers.Conv2DTranspose(32, 3, strides=(2,2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "                layers.Conv2DTranspose(32, 3, strides=(2,2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "                layers.Conv2DTranspose(16, 3, strides=(2,2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "                layers.Conv2DTranspose(16, 3, strides=(2,2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "                layers.Conv2DTranspose(16, 3, strides=(2,2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "                layers.Conv2DTranspose(8, 3, strides=(2,2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "                layers.Conv2D(1, 3, strides=(1,1), padding='same', activation='relu', kernel_initializer='he_normal')\n",
    "            ]\n",
    "        ) \"\"\"\n",
    "        \n",
    "    def crop_output(self):\n",
    "        \n",
    "        h, w = self.decoder.layers[-1].output.get_shape().as_list()[1:3]  # reconstructed width and hight\n",
    "        h_tgt, w_tgt = self.data_size[:2]\n",
    "        dh = h - h_tgt  # deltas to be cropped away\n",
    "        dw = w - w_tgt\n",
    "\n",
    "        # add to decoder cropping layer and final reshaping\n",
    "        self.decoder.add(layers.Cropping2D(cropping=((dh//2, dh-dh//2), (dw//2, dw-dw//2))))\n",
    "        self.decoder.add(layers.Reshape(target_shape=self.output_size,))\n",
    "\n",
    "        \n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
