{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62127006-ca30-4555-9ba3-7e5d614bc7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= '0.20'\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "#from sklearn.pipeline import Pipelines\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "assert tf.__version__ >= '2.0'\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from keras.models import Sequential\n",
    "#from keras.layers.wrappers import TimeDistributed\n",
    "#from keras.layers import Dense,LSTM,Conv2D, BatchNormalization,Flatten, MaxPooling2D\n",
    "#from keras.layers import Conv2DTranspose,Concatenate,UpSampling2D,Cropping2D\n",
    "#from keras.layers import Input, Lambda, Reshape, Dropout, Activation\n",
    "\n",
    "from tensorflow.keras.layers import Dropout, BatchNormalization, Reshape\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Input, MaxPooling2D, Flatten, MaxPool2D, MaxPool3D, UpSampling2D\n",
    "from tensorflow.keras.layers import Conv2DTranspose, Flatten, Reshape, Cropping2D, Embedding, BatchNormalization,ZeroPadding2D\n",
    "from tensorflow.keras.layers import LeakyReLU, Activation, Input, add, multiply\n",
    "from tensorflow.keras.layers import concatenate, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.layers import Lambda\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "#print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# Common imports\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import dask\n",
    "import datetime\n",
    "import math\n",
    "import pathlib\n",
    "import hashlib\n",
    "dask.config.set({'array.slicing.split_large_chunks': False})\n",
    "\n",
    "# To make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# Config matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "# Dotenv\n",
    "from dotenv import dotenv_values\n",
    "# Custom utils\n",
    "from utils.utils_data import *\n",
    "from utils.utils_ml import *\n",
    "from utils.utils_plot import *\n",
    "from utils.utils_unet import *\n",
    "from utils.utils_resnet import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ad39a0-92be-410a-a24a-aae7d51317b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "config = dotenv_values(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3470be0c-2e1a-498c-a199-99f41e5e4445",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_ERA5 = config['PATH_ERA5']\n",
    "PATH_EOBS = config['PATH_EOBS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc151a6-9ce6-489f-806a-0637ef1160f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some constants\n",
    "DATE_START = '1979-01-01'\n",
    "DATE_END = '2021-12-31'\n",
    "# train -test (80/20 of total yy), train-> split 80/20 for validation\n",
    "#YY_TRAIN = [1979, 2011] # 2005-2011 for validation\n",
    "#YY_VAL = [2005,2011]\n",
    "#YY_TEST = [2012, 2020]\n",
    "YY_TRAIN = [1979, 2015]\n",
    "YY_TEST = [2016, 2021]\n",
    "YY_VALID = 2005\n",
    "LEVELS = [500, 850, 1000]\n",
    "G = 9.80665 \n",
    "LONS_INPUT = [-25, 30]\n",
    "LATS_INPUT = [30, 75]\n",
    "LONS_PREC = [-25, 30]\n",
    "LATS_PREC = [30, 75]\n",
    "\n",
    "PRECIP_DATA = 'ERA5' # Options: ERA5, E-OBS\n",
    "PRECIP_XTRM = 0.95 # Percentile (threshold) for the extremes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a970efb-1529-4a59-959f-730911d67693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load precipitation\n",
    "if PRECIP_DATA == 'ERA5':\n",
    "    pr = get_nc_data(PATH_ERA5 + '/precipitation/day_grid1/*nc', DATE_START, DATE_END, LONS_PREC, LATS_PREC)\n",
    "    pr = pr.tp\n",
    "elif PRECIP_DATA in ['E-OBS', 'EOBS']:\n",
    "    pr = get_nc_data(PATH_EOBS + '/*nc', DATE_START, DATE_END, LONS_PREC, LATS_PREC)\n",
    "    pr = pr.rr\n",
    "    pr = pr.coarsen(lon=5, lat=5, boundary=\"trim\").mean()\n",
    "    pr = pr.fillna(0) # Over the seas. Not optimal...\n",
    "else:\n",
    "    raise('Precipitation data not well defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997dec13-bf2d-460d-85a0-3a0ee54b2456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define extremes\n",
    "pr95 = precip_exceedance_xarray(pr, PRECIP_XTRM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7211b02a-777a-4de2-9ccb-7bc5eef42d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coordinates for precip\n",
    "lats_y = pr.lat.to_numpy()\n",
    "lons_y = pr.lon.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e837a7b4-9e7f-4c21-8790-ab9ee40e0483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define paths for the variables used\n",
    "l_paths = ['/geopotential/day_grid1/','/temperature/day_grid1/','/relative_humidity/day_grid1/',\n",
    "              '/U_wind/day_grid1/','/V_wind/day_grid1/','/total_column_water/day_grid1/']\n",
    "v_vars = ['z','t2m','rh','u850','v850','tpcw']\n",
    "list_vars = load_data(v_vars, l_paths, G, PATH_ERA5, DATE_START, DATE_END, LONS_INPUT, LATS_INPUT, LEVELS)\n",
    "datasets = list_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02667306-6f74-470e-bac3-57c35b7a1bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {\n",
    "     'z': LEVELS,\n",
    "      't': LEVELS,\n",
    "      'r': LEVELS,\n",
    "      'u': LEVELS,\n",
    "      'v': LEVELS,\n",
    "      'tcwv':None}\n",
    "# Check if all have the same latitude order\n",
    "for idat in range(0, len(datasets)):\n",
    "    # Invert lat axis if needed\n",
    "    if datasets[idat].lat[0].values < datasets[idat].lat[1].values:\n",
    "        print('change lat order', idat)\n",
    "        datasets[idat] = datasets[idat].reindex(lat=list(reversed(datasets[idat].lat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee1861e-6868-458c-94c8-14499462f6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.merge(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ce6d43-a9f4-48e4-b30a-e537c97287f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=64 # try increase, decrease it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b742f8-42c3-4a74-a252-57d169ce559a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and test, then I will use DataGenerator class to get the validation\n",
    "ds_train = ds.sel(time=slice('{}-01-01'.format(YY_TRAIN[0]),\n",
    "                             '{}-12-31'.format(YY_TRAIN[1])))\n",
    "ds_test = ds.sel(time=slice('{}-01-01'.format(YY_TEST[0]),\n",
    "                            '{}-12-31'.format(YY_TEST[1])))\n",
    "\n",
    "dy_train = pr95.sel(time=slice('{}-01-01'.format(YY_TRAIN[0]),\n",
    "                             '{}-12-31'.format(YY_TRAIN[1])))\n",
    "dy_test = pr95.sel(time=slice('{}-01-01'.format(YY_TEST[0]),\n",
    "                             '{}-12-31'.format(YY_TEST[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a5a412-775b-438f-891e-8e9eab4621d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dy_pr_train = pr.sel(time=slice('{}-01-01'.format(YY_TRAIN[0]),\n",
    "                             '{}-12-31'.format(YY_TRAIN[1])))\n",
    "dy_pr_test = pr.sel(time=slice('{}-01-01'.format(YY_TEST[0]),\n",
    "                             '{}-12-31'.format(YY_TEST[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab748b3-ab85-4ebd-8134-6a3ccdbac4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dg_train = DataGeneratorWithExtremes(ds_train.sel(time=slice(f'{YY_TRAIN[0]}', f'{YY_VALID}')),\n",
    "                                     dy_pr_train.sel(time=slice(f'{YY_TRAIN[0]}', f'{YY_VALID}')),\n",
    "                                     dy_train.sel(time=slice(f'{YY_TRAIN[0]}', f'{YY_VALID}')),\n",
    "                                     dic, batch_size=BATCH_SIZE, load=True)\n",
    "dg_valid = DataGeneratorWithExtremes(ds_train.sel(time=slice(f'{YY_VALID+1}', f'{YY_TRAIN[1]}')),\n",
    "                                     dy_pr_train.sel(time=slice(f'{YY_VALID+1}', f'{YY_TRAIN[1]}')),\n",
    "                                     dy_train.sel(time=slice(f'{YY_VALID+1}', f'{YY_TRAIN[1]}')),\n",
    "                                     dic, mean=dg_train.mean, std=dg_train.std,\n",
    "                                     batch_size=BATCH_SIZE, load=True)\n",
    "dg_test = DataGeneratorWithExtremes(ds_test, dy_pr_test, dy_test, dic,\n",
    "                                    mean=dg_train.mean, std=dg_train.std,\n",
    "                                    batch_size=BATCH_SIZE, load=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b731b4-6118-408a-a5e2-6c831dfbad7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for now I am going to use extremes:\n",
    "# Switch to precipitation extremes\n",
    "dg_train.for_extremes(True)\n",
    "dg_valid.for_extremes(True)\n",
    "dg_test.for_extremes(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793bd805-b698-47d5-9087-9dd5c2d3c7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_shape = dg_train.X.shape[1:]\n",
    "o_shape = dg_train.y.shape[1:]\n",
    "\n",
    "print(f'X shape: {i_shape}')\n",
    "print(f'y shape: {o_shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c949d28b-4aac-48ed-88ed-ca0c1d6775b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = class_weight.compute_class_weight('balanced', classes = np.unique(dg_train.y_xtrm), y = np.array(dg_train.y_xtrm).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b55b7c-47a3-43f0-b469-06881adee8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loss = weighted_binary_cross_entropy(weights = {0: weights[0].astype('float32'), 1: weights[1].astype('float32')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0282eab7-5cea-4135-a26c-f39d09b219fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test GANs ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5397b87a-c759-4487-a7c2-465e0c977b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distankle GAN -class\n",
    "latent_dimension=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c0ef57-40c5-41f5-8fb5-5c6fa9b307ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(img_shape= i_shape):\n",
    "    # Adapted from:\n",
    "    # https://github.com/eriklindernoren/Keras-GAN/blob/master/dcgan/dcgan.py\n",
    "\n",
    "    def conv_block(channels, strides=2):\n",
    "        def block(x):\n",
    "            x = Conv2D(channels, kernel_size=3, strides=strides,\n",
    "                padding=\"same\")(x)\n",
    "            x = LeakyReLU(0.2)(x)\n",
    "            return x\n",
    "        return block\n",
    "\n",
    "    image_in = Input(shape=img_shape, name=\"sample_in\")\n",
    "\n",
    "    x = conv_block(64, strides=1)(image_in)\n",
    "    x = conv_block(128)(x)\n",
    "    x = conv_block(256)(x)\n",
    "    x = Flatten()(x)\n",
    "    disc_out = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model(inputs=image_in, outputs=disc_out)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_generator(img_shape=i_shape, noise_dim=latent_dimension):\n",
    "    # Adapted from:\n",
    "    # https://github.com/eriklindernoren/Keras-GAN/blob/master/dcgan/dcgan.py\n",
    "\n",
    "    def up_block(channels):\n",
    "        def block(x):\n",
    "            x = UpSampling2D()(x)\n",
    "            x = Conv2D(channels, kernel_size=3, padding=\"same\")(x)\n",
    "            x = BatchNormalization(momentum=0.8)(x)\n",
    "            x = LeakyReLU(0.2)(x)\n",
    "            return x\n",
    "        return block\n",
    "\n",
    "    noise_in = Input(shape=(noise_dim,), name=\"noise_in\")\n",
    "    initial_shape = (img_shape[0]//4, img_shape[1]//4, 256)\n",
    "\n",
    "    x = Dense(np.prod(initial_shape))(noise_in)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Reshape(initial_shape)(x)\n",
    "    x = up_block(128)(x)\n",
    "    x = up_block(64)(x)\n",
    "    img_out = Conv2D(img_shape[-1], kernel_size=3, padding=\"same\", \n",
    "        activation=\"tanh\")(x)\n",
    "\n",
    "    return Model(inputs=noise_in, outputs=img_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57c6014-e11a-4464-bc4d-896e2e70f42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build dis\n",
    "dis=build_discriminator(i_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad7d249-4302-4acf-bf83-f241dd39548f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen=build_generator(i_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac15c50-1b3b-4e8c-8593-9dc349bed844",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(0.0002, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582b0f57-d0f5-4cd2-a42c-ccffdb131e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "dis.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d853ed-81ce-4ac6-80e1-b5a586f23396",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = build_generator(i_shape, laten_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b68e1b6-1445-4c32-8d4b-629078e26fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cccf85-c3f8-42c1-a430-822d8b5d862c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class GAN():\n",
    "    def __init__(self, input_s, ouput_size, out_channgels, num_filers, model_loss):\n",
    "        \n",
    "        self.shape_o = ouput_size\n",
    "        self.channels = out_channgels\n",
    "        self.img_shape = input_s\n",
    "        self.latent_dim = num_filers #100 (64)\n",
    "        self.loss = model_loss\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        validity = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, validity)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = X_train / 127.5 - 1.\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            # Generate a batch of new images\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            # Train the generator (to have the discriminator label samples as valid)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.sample_images(epoch)\n",
    "\n",
    "    def sample_images(self, epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"images/%d.png\" % epoch)\n",
    "        plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
