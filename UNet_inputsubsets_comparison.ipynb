{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0b2bbd3-440e-4c52-90fd-51fc4e484f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script to run the main models used to predict precipitation and the extremes\n",
    "# This version uses Keras (instead tensorflow) as in the *ipynb version.\n",
    "\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn â‰¥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= '0.20'\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay, precision_score, recall_score, roc_auc_score, roc_curve\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Common imports\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import dask\n",
    "import datetime\n",
    "import math\n",
    "import pickle\n",
    "import pathlib\n",
    "import hashlib\n",
    "dask.config.set({'array.slicing.split_large_chunks': False})\n",
    "\n",
    "# To make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# Dotenv\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "# Custom utils\n",
    "from utils.utils_data import *\n",
    "from utils.utils_ml import *\n",
    "from utils.utils_resnet import *\n",
    "from utils.utils_plot import *\n",
    "from utils.DNN_models import *\n",
    "\n",
    "config = dotenv_values(\".env\")\n",
    "\n",
    "# Paths\n",
    "PATH_ERA5 = config['PATH_ERA5']\n",
    "PATH_EOBS = config['PATH_EOBS']\n",
    "\n",
    "# Some constants\n",
    "G = 9.80665\n",
    "\n",
    "# Options\n",
    "DATE_START = '1979-01-01'\n",
    "DATE_END = '2021-12-31'\n",
    "YY_TRAIN = [1979, 2015]\n",
    "YY_TEST = [2016, 2021]\n",
    "LEVELS = [300, 500, 700, 850, 925, 1000]\n",
    "LONS_INPUT = [-25, 30]\n",
    "LATS_INPUT = [30, 75]\n",
    "LONS_PREC = [-25, 30]\n",
    "LATS_PREC = [30, 75]\n",
    "BATCH_SIZE = 64\n",
    "PRECIP_DATA = 'ERA5-low' # Options: ERA5-hi, ERA5-low, E-OBS\n",
    "PRECIP_XTRM = 0.95 # Percentile (threshold) for the extremes\n",
    "USE_3D_ONLY = False\n",
    "CREATE_MASK_EOBS = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "764a6a7b-ac45-45d0-8fb2-1d3256e4c4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "conf = yaml.safe_load(open(\"config.yaml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54a16f9e-0603-4e49-9f88-1430b1f79b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data for the period 1979-01-01 - 2021-12-31\n",
      "loading predictors\n",
      "Extracting data for the period 1979-01-01 - 2021-12-31\n",
      "Extracting data for the period 1979-01-01 - 2021-12-31\n",
      "Extracting data for the period 1979-01-01 - 2021-12-31\n",
      "Extracting data for the period 1979-01-01 - 2021-12-31\n",
      "Extracting data for the period 1979-01-01 - 2021-12-31\n",
      "Extracting data for the period 1979-01-01 - 2021-12-31\n"
     ]
    }
   ],
   "source": [
    "# Load precipitation\n",
    "if PRECIP_DATA == 'ERA5-hi':\n",
    "    pr = get_nc_data(PATH_ERA5 + '/precipitation/orig_grid/daily/*nc', DATE_START, DATE_END, LONS_PREC, LATS_PREC)\n",
    "    pr = pr.tp\n",
    "elif PRECIP_DATA == 'ERA5-low':\n",
    "    pr = get_nc_data(PATH_ERA5 + '/precipitation/day_grid1/*nc', DATE_START, DATE_END, LONS_PREC, LATS_PREC)\n",
    "    pr = pr.tp\n",
    "elif PRECIP_DATA in ['E-OBS', 'EOBS']:\n",
    "    pr = get_nc_data(PATH_EOBS + '/eobs_1deg_v26.0e.nc', DATE_START, DATE_END, LONS_PREC, LATS_PREC)\n",
    "    pr = pr.rr\n",
    "    #pr = pr.fillna(0) # Over the seas. Not optimal...\n",
    "else:\n",
    "    raise ValueError('Precipitation data not well defined')\n",
    "\n",
    "    # Add a dimension to be used as channel in the DNN\n",
    "pr = pr.expand_dims('level', -1)\n",
    "\n",
    "\n",
    "# Compute the extreme exceedence\n",
    "qq = xr.DataArray(pr).quantile(PRECIP_XTRM, dim='time')\n",
    "pr_xtrm = xr.DataArray(pr > qq)\n",
    "pr_xtrm = pr_xtrm*1 # Transform to number\n",
    "\n",
    "# Extract coordinates for precip\n",
    "lats_y = pr.lat.to_numpy()\n",
    "lons_y = pr.lon.to_numpy()\n",
    "\n",
    "# Create mask\n",
    "mask = None\n",
    "if CREATE_MASK_EOBS:\n",
    "    if PRECIP_DATA in ['E-OBS', 'EOBS']:\n",
    "        peobs = pr[:,:,:,0]\n",
    "    else:\n",
    "        peobs = get_nc_data(PATH_EOBS + '/eobs_1deg_v26.0e.nc', DATE_START, DATE_END, LONS_PREC, LATS_PREC)\n",
    "        peobs = peobs.rr\n",
    "        if peobs.lat[0].values < peobs.lat[1].values:\n",
    "            peobs = peobs.reindex(lat=list(reversed(peobs.lat)))\n",
    "    mask = np.isnan(peobs[0,:,:])\n",
    "    mask = np.invert(mask)\n",
    "    mask.plot()\n",
    "    mask = mask.to_numpy()\n",
    "    \n",
    "    \n",
    "\n",
    "## Input data: meteorological fields\n",
    "# Load geopotential height\n",
    "print('loading predictors')\n",
    "z = get_era5_data(PATH_ERA5 + '/geopotential/day_grid1/*.nc', DATE_START, DATE_END, LONS_INPUT, LATS_INPUT)\n",
    "z['time'] = pd.DatetimeIndex(z.time.dt.date)\n",
    "z = z.sel(level=LEVELS)\n",
    "\n",
    "# Get Z in geopotential height (m)\n",
    "z.z.values = z.z.values/G\n",
    "\n",
    "# Load temperature\n",
    "t = get_era5_data(PATH_ERA5 + '/temperature/day_grid1/*.nc', DATE_START, DATE_END, LONS_INPUT, LATS_INPUT)\n",
    "t['time'] = pd.DatetimeIndex(t.time.dt.date)\n",
    "\n",
    "# Load relative humidity\n",
    "rh = get_era5_data(PATH_ERA5 + '/relative_humidity/day_grid1/*.nc', DATE_START, DATE_END, LONS_INPUT, LATS_INPUT)\n",
    "rh['time'] = pd.DatetimeIndex(rh.time.dt.date)\n",
    "rh = rh.sel(level=LEVELS)\n",
    "\n",
    "# Load total column water\n",
    "if not USE_3D_ONLY:\n",
    "    tcw = get_era5_data(PATH_ERA5 + '/total_column_water/day_grid1/*.nc', DATE_START, DATE_END, LONS_INPUT, LATS_INPUT)\n",
    "    tcw['time'] = pd.DatetimeIndex(tcw.time.dt.date)\n",
    "\n",
    "# Load wind components\n",
    "u = get_era5_data(PATH_ERA5 + '/U_wind/day_grid1/*.nc', DATE_START, DATE_END, LONS_INPUT, LATS_INPUT)\n",
    "u['time'] = pd.DatetimeIndex(u.time.dt.date)\n",
    "v = get_era5_data(PATH_ERA5 + '/V_wind/day_grid1/*.nc', DATE_START, DATE_END, LONS_INPUT, LATS_INPUT)\n",
    "v['time'] = pd.DatetimeIndex(v.time.dt.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53ea2cfc-3db9-4b16-8aeb-9c1c43d48962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge arrays\n",
    "if USE_3D_ONLY:\n",
    "    X = xr.merge([z, t, rh, u, v])\n",
    "else:\n",
    "    X = xr.merge([z, t, rh, tcw, u, v])\n",
    "\n",
    "# Invert lat axis if needed\n",
    "if X.lat[0].values < X.lat[1].values:\n",
    "    X = X.reindex(lat=list(reversed(X.lat)))\n",
    "    \n",
    "# Get axes\n",
    "lats_x = X.lat\n",
    "lons_x = X.lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9447416-a0b0-4eee-b721-37d511de48c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and test\n",
    "X_train_full = X.sel(time=slice(f'{YY_TRAIN[0]}-01-01', f'{YY_TRAIN[1]}-12-31'))\n",
    "X_test = X.sel(time=slice(f'{YY_TEST[0]}-01-01', f'{YY_TEST[1]}-12-31'))\n",
    "\n",
    "pr_train_full = pr.sel(time=slice(f'{YY_TRAIN[0]}-01-01', f'{YY_TRAIN[1]}-12-31'))\n",
    "pr_test = pr.sel(time=slice(f'{YY_TEST[0]}-01-01', f'{YY_TEST[1]}-12-31'))\n",
    "\n",
    "pr_xtrm_train_full = pr_xtrm.sel(time=slice(f'{YY_TRAIN[0]}-01-01', f'{YY_TRAIN[1]}-12-31'))\n",
    "pr_xtrm_test = pr_xtrm.sel(time=slice(f'{YY_TEST[0]}-01-01', f'{YY_TEST[1]}-12-31'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed5a8280-b6dd-4a74-b25f-3a71f5273e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rain the model for each subset\n",
    "subsets = ['subset_1','subset_2','subset_3','subset_4','subset_5','subset_6']\n",
    "YY_VALID = 2005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21b98375-9a70-4ede-acc5-7bd566fa720e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the necessary output scaling.\n",
    "dlons_x = float(lons_x[1] - lons_x[0])\n",
    "dlats_x = float(lats_x[0] - lats_x[1])\n",
    "dlons_y = float(lons_y[1] - lons_y[0])\n",
    "dlats_y = float(lats_y[0] - lats_y[1])\n",
    "\n",
    "output_scaling = int(dlons_x / dlons_y)\n",
    "output_crop = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a2a05c2-8867-44da-85f9-3b07e871f5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights for the weighted binary crossentropy:\n",
      "Classes: [0 1], weights: [0.52634048 9.99109415]\n"
     ]
    }
   ],
   "source": [
    "# Compute weights for the weighted binary crossentropy\n",
    "weights = class_weight.compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(pr_xtrm.values),\n",
    "    y=pr_xtrm.values.flatten()\n",
    ")\n",
    "\n",
    "print('Weights for the weighted binary crossentropy:')\n",
    "print(f'Classes: {np.unique(pr_xtrm.values)}, weights: {weights}')\n",
    "\n",
    "# Create loss function for the extremes\n",
    "xtrm_loss = weighted_binary_cross_entropy(\n",
    "    weights={0: weights[0].astype('float32'), 1: weights[1].astype('float32')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ad9c0e2-0e33-45e0-89c0-461cd064304e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "EPOCHS = 200\n",
    "LR_METHOD = 'Constant'  # Cyclical, CosineDecay, Constant\n",
    "    \n",
    "# Early stopping\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10,\n",
    "                                            restore_best_weights=True)\n",
    "                                            \n",
    "# Default model options\n",
    "opt_model = {'latent_dim': 128,\n",
    "             'dropout_rate': 0.2}\n",
    "\n",
    "# Default training options\n",
    "opt_training = {'epochs': EPOCHS,\n",
    "                'callbacks': [callback]}\n",
    "\n",
    "# Default optimizer options\n",
    "opt_optimizer = {'lr_method': 'Constant',\n",
    "                 'lr': 0.0004,\n",
    "                 'init_lr': 0.01}\n",
    "loss_regression = 'mse'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91d9b496-83bc-4596-a84f-0bccb72992d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_unets = {\n",
    "          'UNET1': {'model': 'Unet', 'run': False,\n",
    "                   'opt_model': {'output_scaling': output_scaling, 'output_crop': output_crop, 'unet_depth': 1, 'use_upsample': False},\n",
    "                   'opt_optimizer': {'lr_method': 'Constant'}},\n",
    "          'UNET2': {'model': 'Unet', 'run': True,\n",
    "                   'opt_model': {'output_scaling': output_scaling, 'output_crop': output_crop, 'unet_depth': 2, 'use_upsample': False},\n",
    "                   'opt_optimizer': {'lr_method': 'Constant'}},\n",
    "          'UNET3': {'model': 'Unet', 'run': False,\n",
    "                   'opt_model': {'output_scaling': output_scaling, 'output_crop': output_crop, 'unet_depth': 3, 'use_upsample': False},\n",
    "                   'opt_optimizer': {'lr_method': 'Constant'}},\n",
    "          'UNET4': {'model': 'Unet', 'run': False,\n",
    "                   'opt_model': {'output_scaling': output_scaling, 'output_crop': output_crop, 'unet_depth': 4, 'use_upsample': False},\n",
    "                   'opt_optimizer': {'lr_method': 'Constant'}},\n",
    "         }\n",
    "\n",
    "\n",
    "models = models_unets\n",
    "\n",
    "train_for_prec = True\n",
    "train_for_xtrm = True\n",
    "history_log_level = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54d8a71a-564f-4430-92ab-39019fb7d7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the amount of precipitation\n",
    "df_prec = pd.DataFrame(columns = ['id', 'name', 'n_params', 'opt_model', 'opt_optimizer',\n",
    "                                  'train_pr_rmse', 'test_pr_rmse', \n",
    "                                  'train_xtrm_precision', 'test_xtrm_precision', \n",
    "                                  'train_xtrm_recall', 'test_xtrm_recall'])\n",
    "df_xtrm = pd.DataFrame(columns = ['id', 'name', 'n_params', 'opt_model', 'opt_optimizer',\n",
    "                                  'train_xtrm_roc_auc', 'test_xtrm_roc_auc', \n",
    "                                  'train_xtrm_precision', 'test_xtrm_precision', \n",
    "                                  'train_xtrm_recall', 'test_xtrm_recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "18509255-d5b4-433d-a5b0-493af3d47e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_prec = []\n",
    "models_xtrm = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b0834b-936d-4418-b18c-7fbd99067122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r': [700], 'u': [925, 1000], 'v': [925, 1000]}\n",
      "Loading data into RAM\n",
      "Loading data into RAM\n",
      "Loading data into RAM\n",
      "X shape: (46, 56, 5)\n",
      "Running: UNET2 - subset_1 - Unet - {'output_scaling': 1, 'output_crop': None, 'unet_depth': 2, 'use_upsample': False} - {'lr_method': 'Constant'}\n",
      "Number of parameters: 1868865\n",
      "Plotting results of the training period, amount to precipitation.\n",
      "Plotting results of the testing period.\n",
      "id                                                               subset_1\n",
      "name                                                                 Unet\n",
      "n_params                                                          1868865\n",
      "opt_model               {'output_scaling': 1, 'output_crop': None, 'un...\n",
      "opt_optimizer                                   {'lr_method': 'Constant'}\n",
      "train_pr_rmse                                                    1.717345\n",
      "test_pr_rmse                                                     2.109121\n",
      "train_xtrm_precision                                             0.778766\n",
      "test_xtrm_precision                                              0.733385\n",
      "train_xtrm_recall                                                0.608136\n",
      "test_xtrm_recall                                                 0.521949\n",
      "Name: 6, dtype: object\n",
      "\n",
      "****************************************************************************************************\n",
      "\n",
      "Running: UNET2 - subset_1 - Unet - {'output_scaling': 1, 'output_crop': None, 'unet_depth': 2, 'use_upsample': False} - {'lr_method': 'Constant'}\n",
      "Number of parameters: 1868865\n",
      "Plotting results of the training period.\n",
      "Plotting results of the testing period.\n",
      "id                                                               subset_1\n",
      "name                                                                 Unet\n",
      "n_params                                                          1868865\n",
      "opt_model               {'output_scaling': 1, 'output_crop': None, 'un...\n",
      "opt_optimizer                                   {'lr_method': 'Constant'}\n",
      "train_xtrm_roc_auc                                               0.979978\n",
      "test_xtrm_roc_auc                                                0.968381\n",
      "train_xtrm_precision                                              0.33473\n",
      "test_xtrm_precision                                              0.335486\n",
      "train_xtrm_recall                                                0.970531\n",
      "test_xtrm_recall                                                 0.919377\n",
      "Name: 6, dtype: object\n",
      "\n",
      "****************************************************************************************************\n",
      "\n",
      "{'r': [700, 850, 925], 'u': [300, 850, 925, 1000], 'v': [850, 925, 1000]}\n",
      "Loading data into RAM\n",
      "Loading data into RAM\n",
      "Loading data into RAM\n",
      "X shape: (46, 56, 10)\n",
      "Running: UNET2 - subset_2 - Unet - {'output_scaling': 1, 'output_crop': None, 'unet_depth': 2, 'use_upsample': False} - {'lr_method': 'Constant'}\n",
      "Number of parameters: 1871745\n",
      "Plotting results of the training period, amount to precipitation.\n",
      "Plotting results of the testing period.\n",
      "id                                                               subset_2\n",
      "name                                                                 Unet\n",
      "n_params                                                          1871745\n",
      "opt_model               {'output_scaling': 1, 'output_crop': None, 'un...\n",
      "opt_optimizer                                   {'lr_method': 'Constant'}\n",
      "train_pr_rmse                                                    1.552454\n",
      "test_pr_rmse                                                     1.906794\n",
      "train_xtrm_precision                                             0.812396\n",
      "test_xtrm_precision                                              0.780916\n",
      "train_xtrm_recall                                                0.635482\n",
      "test_xtrm_recall                                                 0.558524\n",
      "Name: 7, dtype: object\n",
      "\n",
      "****************************************************************************************************\n",
      "\n",
      "Running: UNET2 - subset_2 - Unet - {'output_scaling': 1, 'output_crop': None, 'unet_depth': 2, 'use_upsample': False} - {'lr_method': 'Constant'}\n",
      "Number of parameters: 1871745\n",
      "Plotting results of the training period.\n",
      "Plotting results of the testing period.\n",
      "id                                                               subset_2\n",
      "name                                                                 Unet\n",
      "n_params                                                          1871745\n",
      "opt_model               {'output_scaling': 1, 'output_crop': None, 'un...\n",
      "opt_optimizer                                   {'lr_method': 'Constant'}\n",
      "train_xtrm_roc_auc                                               0.985718\n",
      "test_xtrm_roc_auc                                                0.976128\n",
      "train_xtrm_precision                                             0.328209\n",
      "test_xtrm_precision                                              0.333939\n",
      "train_xtrm_recall                                                 0.98961\n",
      "test_xtrm_recall                                                 0.952127\n",
      "Name: 7, dtype: object\n",
      "\n",
      "****************************************************************************************************\n",
      "\n",
      "{'r': [500, 700, 850, 925, 1000], 'u': [300, 850, 925, 1000], 'v': [300, 500, 700, 850, 925, 1000]}\n",
      "Loading data into RAM\n",
      "Loading data into RAM\n",
      "Loading data into RAM\n",
      "X shape: (46, 56, 15)\n",
      "Running: UNET2 - subset_3 - Unet - {'output_scaling': 1, 'output_crop': None, 'unet_depth': 2, 'use_upsample': False} - {'lr_method': 'Constant'}\n",
      "Number of parameters: 1874625\n",
      "Plotting results of the training period, amount to precipitation.\n",
      "Plotting results of the testing period.\n",
      "id                                                               subset_3\n",
      "name                                                                 Unet\n",
      "n_params                                                          1874625\n",
      "opt_model               {'output_scaling': 1, 'output_crop': None, 'un...\n",
      "opt_optimizer                                   {'lr_method': 'Constant'}\n",
      "train_pr_rmse                                                    1.487932\n",
      "test_pr_rmse                                                     1.803548\n",
      "train_xtrm_precision                                               0.8314\n",
      "test_xtrm_precision                                              0.806405\n",
      "train_xtrm_recall                                                0.644878\n",
      "test_xtrm_recall                                                 0.578711\n",
      "Name: 8, dtype: object\n",
      "\n",
      "****************************************************************************************************\n",
      "\n",
      "Running: UNET2 - subset_3 - Unet - {'output_scaling': 1, 'output_crop': None, 'unet_depth': 2, 'use_upsample': False} - {'lr_method': 'Constant'}\n",
      "Number of parameters: 1874625\n",
      "Plotting results of the training period.\n",
      "Plotting results of the testing period.\n",
      "id                                                               subset_3\n",
      "name                                                                 Unet\n",
      "n_params                                                          1874625\n",
      "opt_model               {'output_scaling': 1, 'output_crop': None, 'un...\n",
      "opt_optimizer                                   {'lr_method': 'Constant'}\n",
      "train_xtrm_roc_auc                                               0.987122\n",
      "test_xtrm_roc_auc                                                0.979406\n",
      "train_xtrm_precision                                             0.369299\n",
      "test_xtrm_precision                                              0.374759\n",
      "train_xtrm_recall                                                0.985058\n",
      "test_xtrm_recall                                                 0.947768\n",
      "Name: 8, dtype: object\n",
      "\n",
      "****************************************************************************************************\n",
      "\n",
      "{'r': [300, 500, 700, 850, 925, 1000], 'u': [300, 500, 700, 850, 925, 1000], 'v': [300, 500, 700, 850, 925, 1000], 'tcwv': None, 't': [1000]}\n",
      "Loading data into RAM\n",
      "Loading data into RAM\n",
      "Loading data into RAM\n",
      "X shape: (46, 56, 20)\n",
      "Running: UNET2 - subset_4 - Unet - {'output_scaling': 1, 'output_crop': None, 'unet_depth': 2, 'use_upsample': False} - {'lr_method': 'Constant'}\n",
      "Number of parameters: 1877505\n",
      "Plotting results of the training period, amount to precipitation.\n",
      "Plotting results of the testing period.\n",
      "id                                                               subset_4\n",
      "name                                                                 Unet\n",
      "n_params                                                          1877505\n",
      "opt_model               {'output_scaling': 1, 'output_crop': None, 'un...\n",
      "opt_optimizer                                   {'lr_method': 'Constant'}\n",
      "train_pr_rmse                                                     1.41289\n",
      "test_pr_rmse                                                     1.704703\n",
      "train_xtrm_precision                                             0.833417\n",
      "test_xtrm_precision                                              0.803993\n",
      "train_xtrm_recall                                                0.672021\n",
      "test_xtrm_recall                                                  0.62209\n",
      "Name: 9, dtype: object\n",
      "\n",
      "****************************************************************************************************\n",
      "\n",
      "Running: UNET2 - subset_4 - Unet - {'output_scaling': 1, 'output_crop': None, 'unet_depth': 2, 'use_upsample': False} - {'lr_method': 'Constant'}\n",
      "Number of parameters: 1877505\n",
      "Plotting results of the training period.\n",
      "Plotting results of the testing period.\n",
      "id                                                               subset_4\n",
      "name                                                                 Unet\n",
      "n_params                                                          1877505\n",
      "opt_model               {'output_scaling': 1, 'output_crop': None, 'un...\n",
      "opt_optimizer                                   {'lr_method': 'Constant'}\n",
      "train_xtrm_roc_auc                                               0.988145\n",
      "test_xtrm_roc_auc                                                0.981767\n",
      "train_xtrm_precision                                              0.38866\n",
      "test_xtrm_precision                                              0.387246\n",
      "train_xtrm_recall                                                 0.98425\n",
      "test_xtrm_recall                                                  0.95363\n",
      "Name: 9, dtype: object\n",
      "\n",
      "****************************************************************************************************\n",
      "\n",
      "{'r': [300, 500, 700, 850, 925, 1000], 'u': [300, 500, 700, 850, 925, 1000], 'v': [300, 500, 700, 850, 925, 1000], 'tcwv': None, 't': [925, 1000], 'z': [700, 850, 925, 1000]}\n",
      "Loading data into RAM\n",
      "Loading data into RAM\n",
      "Loading data into RAM\n",
      "X shape: (46, 56, 25)\n",
      "Running: UNET2 - subset_5 - Unet - {'output_scaling': 1, 'output_crop': None, 'unet_depth': 2, 'use_upsample': False} - {'lr_method': 'Constant'}\n",
      "Number of parameters: 1880385\n"
     ]
    }
   ],
   "source": [
    "for sub in subsets:\n",
    "    \n",
    "    print(conf[sub])\n",
    "    dic = conf[sub]\n",
    "    \n",
    "    if 'tcwv' in dic.keys(): # make sure to convert to nontype\n",
    "        dic['tcwv']=None\n",
    "    # apply datagenerator\n",
    "    dg_train = DataGeneratorWithExtremes(X_train_full.sel(time=slice(f'{YY_TRAIN[0]}', f'{YY_VALID}')),\n",
    "                                     pr_train_full.sel(time=slice(f'{YY_TRAIN[0]}', f'{YY_VALID}')),\n",
    "                                     pr_xtrm_train_full.sel(time=slice(f'{YY_TRAIN[0]}', f'{YY_VALID}')),\n",
    "                                     dic, batch_size=BATCH_SIZE, load=True)\n",
    "    dg_valid = DataGeneratorWithExtremes(X_train_full.sel(time=slice(f'{YY_VALID+1}', f'{YY_TRAIN[1]}')),\n",
    "                                         pr_train_full.sel(time=slice(f'{YY_VALID+1}', f'{YY_TRAIN[1]}')),\n",
    "                                         pr_xtrm_train_full.sel(time=slice(f'{YY_VALID+1}', f'{YY_TRAIN[1]}')),\n",
    "                                         dic, mean=dg_train.mean, std=dg_train.std,\n",
    "                                         batch_size=BATCH_SIZE, load=True)\n",
    "    dg_test = DataGeneratorWithExtremes(X_test, pr_test, pr_xtrm_test, dic,\n",
    "                                        mean=dg_train.mean, std=dg_train.std,\n",
    "                                        batch_size=BATCH_SIZE, load=True, shuffle=False)\n",
    "\n",
    "\n",
    "    i_shape = dg_train.X.shape[1:]\n",
    "    o_shape = dg_train.y.shape[1:]\n",
    "\n",
    "    print(f'X shape: {i_shape}')\n",
    "\n",
    "    if train_for_prec:\n",
    "        \n",
    "        for m_id in models:\n",
    "            # Clear session and set tf seed\n",
    "            keras.backend.clear_session()\n",
    "            tf.random.set_seed(42)\n",
    "\n",
    "            if not models[m_id]['run']:\n",
    "                continue\n",
    "\n",
    "            # Extract model name and options\n",
    "            model = models[m_id]['model']\n",
    "            opt_model_i = models[m_id]['opt_model']\n",
    "            opt_optimizer_i = models[m_id]['opt_optimizer']\n",
    "            opt_model_new = opt_model.copy()\n",
    "            opt_model_new.update(opt_model_i)\n",
    "            opt_optimizer_new = opt_optimizer.copy()\n",
    "            opt_optimizer_new.update(opt_optimizer_i)\n",
    "            print(f'Running: {m_id} - {sub} - {model} - {opt_model_i} - {opt_optimizer_i}')\n",
    "            df_prec = df_prec.append({'id': sub, 'name': model, 'opt_model': opt_model_i, 'opt_optimizer': opt_optimizer_i}, ignore_index=True)\n",
    "\n",
    "            # Switch to precipitation values\n",
    "            dg_train.for_extremes(False)\n",
    "            dg_valid.for_extremes(False)\n",
    "            dg_test.for_extremes(False)\n",
    "\n",
    "            optimizer = initiate_optimizer(**opt_optimizer_new)\n",
    "\n",
    "            # Load if previously saved\n",
    "            tag = pickle.dumps(opt_model_new) + pickle.dumps(opt_optimizer_new) + \\\n",
    "                pickle.dumps(i_shape + o_shape) + pickle.dumps(PRECIP_XTRM)\n",
    "            hashed_name = f'prec_{m_id}_{hashlib.md5(tag).hexdigest()}'\n",
    "            tmp_file = pathlib.Path(f'tmp/{hashed_name}')\n",
    "\n",
    "            if tmp_file.exists():\n",
    "                m = keras.models.load_model(tmp_file, custom_objects = {\"MeanSquaredErrorNans\": MeanSquaredErrorNans})\n",
    "\n",
    "            else:\n",
    "                # Create the model and compile\n",
    "                m = DeepFactory_Keras(model, i_shape, o_shape, for_extremes=False, **opt_model_new)\n",
    "                # Warning: When using regularizers, the loss function is the entire loss, ie (loss metrics) + (regularization term)!\n",
    "                # But the loss displayed as part of the metrics, is only the loss metric. The regularization term is not added there. -> can be different!!\n",
    "                loss_fct = 'mse'\n",
    "                if loss_regression == 'mse_nans':\n",
    "                    loss_fct = MeanSquaredErrorNans()\n",
    "                m.model.compile(\n",
    "                    loss=loss_fct, \n",
    "                    metrics=[loss_fct], \n",
    "                    optimizer=optimizer,\n",
    "                )\n",
    "                print(f'Number of parameters: {m.model.count_params()}')\n",
    "\n",
    "                # Train\n",
    "                hist = m.model.fit(dg_train, validation_data=dg_valid, verbose=history_log_level, **opt_training)\n",
    "\n",
    "             \n",
    "                # Saving the model\n",
    "                m.model.save_weights(f'tmp/keras/{PRECIP_DATA}_{PRECIP_XTRM}_{m_id}_{sub}.h5')\n",
    "\n",
    "            df_prec.at[df_prec.index[-1], 'n_params'] = m.model.count_params()\n",
    "            models_prec.append(m)\n",
    "        \n",
    "            # Predict and save scores\n",
    "            y_pred_train = m.model.predict(dg_train.X.to_numpy()).squeeze()\n",
    "            y_pred_test = m.model.predict(dg_test.X.to_numpy()).squeeze()\n",
    "            df_prec.at[df_prec.index[-1], 'train_pr_rmse'] = np.sqrt(np.nanmean(np.square(np.subtract(dg_train.y.to_numpy().squeeze(), y_pred_train))))\n",
    "            df_prec.at[df_prec.index[-1], 'test_pr_rmse'] =  np.sqrt(np.nanmean(np.square(np.subtract(dg_test.y.to_numpy().squeeze(), y_pred_test))))\n",
    "\n",
    "            # Analyze predictions\n",
    "            print('Plotting results of the training period, amount to precipitation.')\n",
    "            precision, recall = analyze_predictions(y_pred_train, dg_train, qq, lons_y, lats_y, mask=mask, pred_xtrm=False, show_plots=False)\n",
    "            df_prec.at[df_prec.index[-1], 'train_xtrm_precision'] = precision\n",
    "            df_prec.at[df_prec.index[-1], 'train_xtrm_recall'] = recall\n",
    "            plt.show()\n",
    "\n",
    "            print('Plotting results of the testing period.')\n",
    "            precision, recall = analyze_predictions(y_pred_test, dg_test, qq, lons_y, lats_y, mask=mask, pred_xtrm=False,  show_plots=False)\n",
    "            df_prec.at[df_prec.index[-1], 'test_xtrm_precision'] = precision\n",
    "            df_prec.at[df_prec.index[-1], 'test_xtrm_recall'] = recall\n",
    "            plt.show()\n",
    "\n",
    "            print(df_prec.iloc[-1])\n",
    "\n",
    "            print(f\"\\n{'*' * 100}\\n\")\n",
    "\n",
    "        if train_for_xtrm:\n",
    "\n",
    "            for m_id in models:\n",
    "                # Clear session and set tf seed\n",
    "                keras.backend.clear_session()\n",
    "                tf.random.set_seed(42)\n",
    "\n",
    "                if not models[m_id]['run']:\n",
    "                    continue\n",
    "\n",
    "                # Extract model name and options\n",
    "                model = models[m_id]['model']\n",
    "                opt_model_i = models[m_id]['opt_model']\n",
    "                opt_optimizer_i = models[m_id]['opt_optimizer']\n",
    "                opt_model_new = opt_model.copy()\n",
    "                opt_model_new.update(opt_model_i)\n",
    "                opt_optimizer_new = opt_optimizer.copy()\n",
    "                opt_optimizer_new.update(opt_optimizer_i)\n",
    "                print(f'Running: {m_id} - {sub} - {model} - {opt_model_i} - {opt_optimizer_i}')\n",
    "                df_xtrm = df_xtrm.append({'id': sub, 'name': model, 'opt_model': opt_model_i, 'opt_optimizer': opt_optimizer_i}, ignore_index=True)\n",
    "\n",
    "                # Switch to precipitation extremes\n",
    "                dg_train.for_extremes(True)\n",
    "                dg_valid.for_extremes(True)\n",
    "                dg_test.for_extremes(True)\n",
    "\n",
    "                optimizer = initiate_optimizer(**opt_optimizer_new)\n",
    "\n",
    "                # Load if previously saved\n",
    "                tag = pickle.dumps(opt_model_new) + pickle.dumps(opt_optimizer_new) + \\\n",
    "                    pickle.dumps(i_shape + o_shape) + pickle.dumps(PRECIP_XTRM)\n",
    "                hashed_name = f'xtrm_{m_id}_{hashlib.md5(tag).hexdigest()}'\n",
    "                tmp_file = pathlib.Path(f'tmp/{hashed_name}')\n",
    "\n",
    "                if tmp_file.exists():\n",
    "                    #custom_objects = {\"weighted_cross_entropy_fn\": weighted_binary_cross_entropy}\n",
    "                    #with keras.utils.custom_object_scope(custom_objects):\n",
    "                        m = keras.models.load_model(tmp_file)\n",
    "\n",
    "                else:\n",
    "                    # Create the model and compile\n",
    "                    m = DeepFactory_Keras(model, i_shape, o_shape, for_extremes=True, **opt_model_new)\n",
    "                    m.model.compile(\n",
    "                        loss=xtrm_loss,\n",
    "                        optimizer=optimizer\n",
    "                    )\n",
    "                    print(f'Number of parameters: {m.model.count_params()}')\n",
    "\n",
    "                    # Train\n",
    "                    hist = m.model.fit(dg_train, validation_data=dg_valid, verbose=history_log_level, **opt_training)\n",
    "\n",
    "                    # Plot training evolution\n",
    "                    #pd.DataFrame(hist.history).plot(figsize=(8, 5))\n",
    "                    #ymin = 0.8 * min([min(hist.history['loss']), min(hist.history['val_loss'])])\n",
    "                    #ymax = min(max(max(hist.history['loss']), max(hist.history['val_loss'])), \n",
    "                    #           2 * max(sum(hist.history['loss'])/len(hist.history['loss']), sum(hist.history['val_loss'])/len(hist.history['val_loss'])))\n",
    "                    #plt.ylim(ymin, ymax)\n",
    "                    #plt.grid(True)\n",
    "                    #plt.title(model)\n",
    "                    #plt.tight_layout()\n",
    "                    #now = datetime.datetime.now()\n",
    "                    #plt.savefig(f'figures/training_xtrm_{m_id}_{now.strftime(\"%Y-%m-%d_%H-%M-%S\")}.pdf')\n",
    "                    #plt.savefig(f'figures/training_xtrm_{m_id}_{now.strftime(\"%Y-%m-%d_%H-%M-%S\")}.png')\n",
    "                    #plt.show()\n",
    "\n",
    "                    # Saving the model\n",
    "                    m.model.save_weights(f'tmp/keras/{PRECIP_DATA}_{PRECIP_XTRM}_{m_id}_{sub}_xtrm.h5')\n",
    "\n",
    "                df_xtrm.at[df_xtrm.index[-1], 'n_params'] = m.model.count_params()\n",
    "                models_xtrm.append(m)\n",
    "\n",
    "                # Assess and save scores\n",
    "                y_pred_train = m.model.predict(dg_train.X.to_numpy()).squeeze()\n",
    "                y_pred_test = m.model.predict(dg_test.X.to_numpy()).squeeze()\n",
    "\n",
    "                # save predictions\n",
    "                #np.save(f'tmp/data/predictions/y_pred_train_xtrm_{m_id}.npy',y_pred_train)\n",
    "                #np.save(f'tmp/data/predictions/y_pred_test_xtrm_{m_id}.npy',y_pred_test)\n",
    "\n",
    "                #Analyze predictions\n",
    "                print('Plotting results of the training period.')\n",
    "                precision, recall, roc_auc = analyze_predictions(y_pred_train, dg_train, qq, lons_y, lats_y, mask=mask, pred_xtrm=True, show_plots=False)\n",
    "                df_xtrm.at[df_xtrm.index[-1], 'train_xtrm_precision'] = precision\n",
    "                df_xtrm.at[df_xtrm.index[-1], 'train_xtrm_recall'] = recall\n",
    "                df_xtrm.at[df_xtrm.index[-1], 'train_xtrm_roc_auc'] = roc_auc\n",
    "                plt.show()\n",
    "\n",
    "                print('Plotting results of the testing period.')\n",
    "                precision, recall, roc_auc = analyze_predictions(y_pred_test, dg_test, qq, lons_y, lats_y, mask=mask, pred_xtrm=True, show_plots=False)\n",
    "                df_xtrm.at[df_xtrm.index[-1], 'test_xtrm_precision'] = precision\n",
    "                df_xtrm.at[df_xtrm.index[-1], 'test_xtrm_recall'] = recall\n",
    "                df_xtrm.at[df_xtrm.index[-1], 'test_xtrm_roc_auc'] = roc_auc\n",
    "                plt.show()\n",
    "\n",
    "                print(df_xtrm.iloc[-1])\n",
    "\n",
    "                print(f\"\\n{'*' * 100}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf16813a-d996-40a1-8372-804d0c104331",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
