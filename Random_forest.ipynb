{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d545b75d",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "### Import necessary modules and do some basic setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1427bd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= '0.20'\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay, precision_score, recall_score, roc_auc_score, roc_curve\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Common imports\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import dask\n",
    "import datetime\n",
    "import math\n",
    "import pickle\n",
    "import pathlib\n",
    "import hashlib\n",
    "#dask.config.set({'array.slicing.split_large_chunks': False})\n",
    "\n",
    "# To make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# Config matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Dotenv\n",
    "from dotenv import dotenv_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8daf9b9-8e6b-44e3-9763-11942a521ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom utils\n",
    "from utils.utils_data import *\n",
    "from utils.utils_plot import *\n",
    "#from utils.utils_ml import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41766d4a",
   "metadata": {},
   "source": [
    "### Define some paths and constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc1d02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dotenv_values(\".env\")\n",
    "\n",
    "# Paths\n",
    "PATH_ERA5 = config['PATH_ERA5']\n",
    "\n",
    "# Some constants\n",
    "G = 9.80665\n",
    "\n",
    "# Options\n",
    "DATE_START = '1979-01-01'\n",
    "DATE_END = '2021-12-31'\n",
    "YY_TRAIN = [1979, 2015]\n",
    "YY_TEST = [2016, 2021]\n",
    "LEVELS = [300, 500, 700, 850, 925, 1000]\n",
    "LONS_INPUT = [-25, 30]\n",
    "LATS_INPUT = [30, 75]\n",
    "LONS_PREC = [-25, 30]\n",
    "LATS_PREC = [30, 75]\n",
    "BATCH_SIZE = 64\n",
    "PRECIP_XTRM = 0.99 # Percentile (threshold) for the extremes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98c274e-b72a-43c9-9ca0-be7cc16f1ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsample domain for dev\n",
    "LONS_INPUT = [10, 15]\n",
    "LATS_INPUT = [40, 45]\n",
    "LONS_PREC = [10, 15]\n",
    "LATS_PREC = [40, 45]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c3dbae",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74596028",
   "metadata": {},
   "source": [
    "## Target variable: precipitation field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abeea2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load precipitation\n",
    "pr = get_nc_data(PATH_ERA5 + '/precipitation/day_grid1/*nc', DATE_START, DATE_END, LONS_PREC, LATS_PREC)\n",
    "pr = pr.tp\n",
    "pr['time'] = pd.DatetimeIndex(pr.time.dt.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a9308b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the extreme exceedence\n",
    "qq = xr.DataArray(pr).chunk(dict(time=-1)).quantile(PRECIP_XTRM, dim='time')\n",
    "pr_xtrm = xr.DataArray(pr > qq)\n",
    "pr_xtrm = pr_xtrm*1 # Transform to number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd208aba-31eb-42f2-a1b0-6d643b2ceadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coordinates for precip\n",
    "lats_y = pr.lat.to_numpy()\n",
    "lons_y = pr.lon.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c847649a",
   "metadata": {},
   "source": [
    "## Input data: meteorological fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce5176d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "    # Load geopotential height\n",
    "    z = get_era5_data(PATH_ERA5 + '/geopotential/day_grid1/*.nc', DATE_START, DATE_END, LONS_INPUT, LATS_INPUT)\n",
    "    z['time'] = pd.DatetimeIndex(z.time.dt.date)\n",
    "    z = z.sel(level=LEVELS)\n",
    "\n",
    "    # Get Z in geopotential height (m)\n",
    "    z.z.values = z.z.values/G\n",
    "\n",
    "    # Load temperature\n",
    "    t = get_era5_data(PATH_ERA5 + '/temperature/day_grid1/*.nc', DATE_START, DATE_END, LONS_INPUT, LATS_INPUT)\n",
    "    t['time'] = pd.DatetimeIndex(t.time.dt.date)\n",
    "\n",
    "    # Load relative humidity\n",
    "    rh = get_era5_data(PATH_ERA5 + '/relative_humidity/day_grid1/*.nc', DATE_START, DATE_END, LONS_INPUT, LATS_INPUT)\n",
    "    rh['time'] = pd.DatetimeIndex(rh.time.dt.date)\n",
    "    rh = rh.sel(level=LEVELS)\n",
    "\n",
    "    # Load total column water\n",
    "    tcw = get_era5_data(PATH_ERA5 + '/total_column_water/day_grid1/*.nc', DATE_START, DATE_END, LONS_INPUT, LATS_INPUT)\n",
    "    tcw['time'] = pd.DatetimeIndex(tcw.time.dt.date)\n",
    "\n",
    "    # Load wind components\n",
    "    u = get_era5_data(PATH_ERA5 + '/U_wind/day_grid1/*.nc', DATE_START, DATE_END, LONS_INPUT, LATS_INPUT)\n",
    "    u['time'] = pd.DatetimeIndex(u.time.dt.date)\n",
    "    v = get_era5_data(PATH_ERA5 + '/V_wind/day_grid1/*.nc', DATE_START, DATE_END, LONS_INPUT, LATS_INPUT)\n",
    "    v['time'] = pd.DatetimeIndex(v.time.dt.date)\n",
    "\n",
    "# Checking dimensions\n",
    "print('dimension of pr:', pr.dims)\n",
    "print('dimension of z', z.dims)\n",
    "print('dimension of t:', t.dims)\n",
    "print('dimension of rh:', rh.dims)\n",
    "print('dimension of tcw:', tcw.dims)\n",
    "print('dimension of u:', u.dims)\n",
    "print('dimension of v:', v.dims)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b82f636-3da7-4414-9619-0a57058b7c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge arrays\n",
    "with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "    X = xr.merge([z, t, rh, tcw, u, v])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a200bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert lat axis if needed\n",
    "if X.lat[0].values < X.lat[1].values:\n",
    "    X = X.reindex(lat=list(reversed(X.lat)))\n",
    "    \n",
    "# Get axes\n",
    "lats_x = X.lat\n",
    "lons_x = X.lon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7643ef",
   "metadata": {},
   "source": [
    "### Split data and transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23738ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and test\n",
    "X_train_full = X.sel(time=slice(f'{YY_TRAIN[0]}-01-01', f'{YY_TRAIN[1]}-12-31'))\n",
    "X_test = X.sel(time=slice(f'{YY_TEST[0]}-01-01', f'{YY_TEST[1]}-12-31'))\n",
    "\n",
    "pr_train_full = pr.sel(time=slice(f'{YY_TRAIN[0]}-01-01', f'{YY_TRAIN[1]}-12-31'))\n",
    "pr_test = pr.sel(time=slice(f'{YY_TEST[0]}-01-01', f'{YY_TEST[1]}-12-31'))\n",
    "\n",
    "pr_xtrm_train_full = pr_xtrm.sel(time=slice(f'{YY_TRAIN[0]}-01-01', f'{YY_TRAIN[1]}-12-31'))\n",
    "pr_xtrm_test = pr_xtrm.sel(time=slice(f'{YY_TEST[0]}-01-01', f'{YY_TEST[1]}-12-31'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be5e320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data generator\n",
    "dic = {'z': LEVELS,\n",
    "   't': LEVELS,\n",
    "   'r': LEVELS,\n",
    "   'tcwv': None,\n",
    "   'u': LEVELS,\n",
    "   'v': LEVELS}\n",
    "\n",
    "from utils.utils_ml import *\n",
    "\n",
    "YY_VALID = 2005\n",
    "\n",
    "dg_train = DataGeneratorWithExtremes(X_train_full.sel(time=slice(f'{YY_TRAIN[0]}', f'{YY_VALID}')),\n",
    "                                     pr_train_full.sel(time=slice(f'{YY_TRAIN[0]}', f'{YY_VALID}')),\n",
    "                                     pr_xtrm_train_full.sel(time=slice(f'{YY_TRAIN[0]}', f'{YY_VALID}')),\n",
    "                                     dic, batch_size=BATCH_SIZE, load=True)\n",
    "dg_valid = DataGeneratorWithExtremes(X_train_full.sel(time=slice(f'{YY_VALID+1}', f'{YY_TRAIN[1]}')),\n",
    "                                     pr_train_full.sel(time=slice(f'{YY_VALID+1}', f'{YY_TRAIN[1]}')),\n",
    "                                     pr_xtrm_train_full.sel(time=slice(f'{YY_VALID+1}', f'{YY_TRAIN[1]}')),\n",
    "                                     dic, mean=dg_train.mean, std=dg_train.std,\n",
    "                                     batch_size=BATCH_SIZE, load=True)\n",
    "dg_test = DataGeneratorWithExtremes(X_test, pr_test, pr_xtrm_test, dic,\n",
    "                                    mean=dg_train.mean, std=dg_train.std,\n",
    "                                    batch_size=BATCH_SIZE, load=True, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429afe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_shape = dg_train.X.shape\n",
    "o_shape = dg_train.y.shape\n",
    "\n",
    "print(f'X shape: {i_shape}')\n",
    "print(f'y shape: {o_shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1538787e-2f13-4e04-9581-2ce873c62fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk = {'lon':3, 'lat':3}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ba7bd3",
   "metadata": {},
   "source": [
    "# Train a random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7d6826-224c-4090-818e-9aa973861bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rf_classifier_model(X, y):\n",
    "    try:\n",
    "        clf = RandomForestClassifier(max_depth=4, random_state=42, class_weight='balanced')\n",
    "        clf.fit(X, y)\n",
    "        print('|', end='')\n",
    "        return clf\n",
    "    except:\n",
    "        print('Failed to create the model')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003c9022-770d-434b-8fb4-ee14bfb2dbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rf_classifier_model(clf, X):\n",
    "    try:\n",
    "        print('|', end='')\n",
    "        return clf.predict_proba(X) #[:,0]\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb56db2-7662-47a1-bacd-12dbae977e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dask = dg_train.X.chunk(chunk)\n",
    "y_train_dask = dg_train.y_xtrm.chunk(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dfc514-21a0-4ae0-b230-f169e28edd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "crfs = xr.apply_ufunc(\n",
    "    train_rf_classifier_model,\n",
    "    X_train_dask, y_train_dask,\n",
    "    vectorize=True,\n",
    "    dask = 'parallelized',\n",
    "    input_core_dims=[['time', 'level'], ['time']], # reduce along these dimensions\n",
    ").compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6855818f-c5c2-4f20-931c-f145ce366048",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "preds = xr.apply_ufunc(\n",
    "    apply_rf_classifier_model, \n",
    "    crfs, dg_test.X,\n",
    "    vectorize=True,\n",
    "    input_core_dims=[[],['time', 'level']],\n",
    "    output_dtypes=[object]\n",
    ").compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69da6fae-26cb-426b-a00d-2ad1ec02a08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape predictions to create maps\n",
    "preds_reshaped = np.zeros(dg_test.y.shape)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a7387b-797d-4ed1-98a2-13b15e85fcff",
   "metadata": {},
   "source": [
    "# Train a random forest regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d57a0a-6362-46ee-bfa9-a3a32265b998",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8654739-cb2c-412d-8c51-ca61ea295408",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3e58ba8-5307-489c-9a92-25b2b7e0bbd1",
   "metadata": {},
   "source": [
    "# Predict and assess on the test period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a7ed93-3060-473b-ad44-a0e45b3eda77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_predictions(y_pred, dg, qq, pred_xtrm=False, show_plots=True, plot_most_extreme=True, plot_worst_best=False, plot_scores=True, plot_confusion_components=True):\n",
    "    if pred_xtrm:\n",
    "        y_pred_bool = y_pred >= 0.5\n",
    "    else:\n",
    "        y_pred_bool = y_pred > qq.to_numpy().squeeze()\n",
    "        \n",
    "    # Multiply to transorm to numeric values\n",
    "    y_pred_bool = y_pred_bool * 1\n",
    "    \n",
    "    # Extract true values\n",
    "    y_xtrm = dg.y_xtrm.to_numpy().squeeze()\n",
    "    y_prec = dg.y.to_numpy().squeeze()\n",
    "    \n",
    "    # Get the index of the max # of extremes\n",
    "    i_max_obs = np.argmax(np.sum(y_xtrm, axis=(1,2)))\n",
    "    \n",
    "    if show_plots and plot_most_extreme:\n",
    "        if pred_xtrm:\n",
    "            fig, axes = plt.subplots(figsize=(12, 3.5), ncols=2, nrows=1)\n",
    "            plot_map(axes[0], lons_y, lats_y, y_xtrm[i_max_obs], title=\"Day with max # extremes - truth (xtrm)\", vmin=0, vmax=1)\n",
    "            plot_map(axes[1], lons_y, lats_y, y_pred[i_max_obs], title=\"Day with max # extremes - prediction (prob xtrm)\", vmin=0, vmax=1)\n",
    "        else:\n",
    "            fig, axes = plt.subplots(figsize=(24, 3.5), ncols=4, nrows=1)\n",
    "            vmax = max(np.max(y_prec[i_max_obs]), np.max(y_pred[i_max_obs]))\n",
    "            plot_map(axes[0], lons_y, lats_y, y_prec[i_max_obs], title=\"Day with max # extremes - truth (val)\", vmin=0, vmax=vmax)\n",
    "            plot_map(axes[1], lons_y, lats_y, y_pred[i_max_obs], title=\"Day with max # extremes - prediction (val)\", vmin=0, vmax=vmax)\n",
    "            plot_map(axes[2], lons_y, lats_y, y_xtrm[i_max_obs], title=\"Day with max # extremes - truth (xtrm)\", vmin=0, vmax=1)\n",
    "            plot_map(axes[3], lons_y, lats_y, y_pred_bool[i_max_obs], title=\"Day with max # extremes - prediction (xtrm)\", vmin=0, vmax=1)\n",
    "        \n",
    "    # Get the index of the max/min difference between prediction and obs\n",
    "    y_diffs_series = np.sum(np.absolute(y_xtrm - y_pred_bool), axis=(1,2))\n",
    "    i_worst_pred = np.argmax(y_diffs_series)\n",
    "    i_best_pred = np.argmin(y_diffs_series)\n",
    "    \n",
    "    if show_plots and plot_worst_best:\n",
    "        if pred_xtrm:\n",
    "            fig, axes = plt.subplots(figsize=(24, 3.5), ncols=4, nrows=1)\n",
    "            plot_map(axes[0], lons_y, lats_y, y_xtrm[i_worst_pred], title=\"Worst prediction - truth\", vmin=0, vmax=1)\n",
    "            plot_map(axes[1], lons_y, lats_y, y_pred[i_worst_pred], title=\"Worst prediction - prediction\", vmin=0, vmax=1)\n",
    "            plot_map(axes[2], lons_y, lats_y, y_xtrm[i_best_pred], title=\"Best prediction - truth\", vmin=0, vmax=1)\n",
    "            plot_map(axes[3], lons_y, lats_y, y_pred[i_best_pred], title=\"Best prediction - prediction\", vmin=0, vmax=1)\n",
    "        else:\n",
    "            fig, axes = plt.subplots(figsize=(24, 3.5), ncols=4, nrows=1)\n",
    "            plot_map(axes[0], lons_y, lats_y, y_xtrm[i_worst_pred], title=\"Worst prediction - truth\", vmin=0, vmax=1)\n",
    "            plot_map(axes[1], lons_y, lats_y, y_pred_bool[i_worst_pred], title=\"Worst prediction - prediction\", vmin=0, vmax=1)\n",
    "            plot_map(axes[2], lons_y, lats_y, y_xtrm[i_best_pred], title=\"Best prediction - truth\", vmin=0, vmax=1)\n",
    "            plot_map(axes[3], lons_y, lats_y, y_pred_bool[i_best_pred], title=\"Best prediction - prediction\", vmin=0, vmax=1)\n",
    "    \n",
    "    # Compute scores\n",
    "    precision, recall = eval_confusion_matrix_scores_on_map(y_xtrm, y_pred_bool)\n",
    "\n",
    "    if pred_xtrm:\n",
    "        roc_auc = eval_roc_auc_score_on_map(y_xtrm, y_pred)\n",
    "        \n",
    "    if show_plots and plot_scores:\n",
    "        if pred_xtrm:\n",
    "            fig, axes = plt.subplots(figsize=(18, 4), ncols=3, nrows=1)\n",
    "        else:\n",
    "            fig, axes = plt.subplots(figsize=(12, 4), ncols=2, nrows=1)\n",
    "\n",
    "        plot_map(axes[0], lons_y, lats_y, precision, title=\"Precision\", vmin=0, vmax=1)\n",
    "        plot_map(axes[1], lons_y, lats_y, recall, title=\"Recall\", vmin=0, vmax=1)\n",
    "        if pred_xtrm:\n",
    "            plot_map(axes[2], lons_y, lats_y, roc_auc, title=\"ROC AUC\", vmin=0.5, vmax=1)\n",
    "\n",
    "    if show_plots and plot_confusion_components:\n",
    "        tn, fp, fn, tp = eval_confusion_matrix_on_map(y_xtrm, y_pred_bool)\n",
    "        fig, axes = plt.subplots(figsize=(24, 4), ncols=4, nrows=1)\n",
    "        plot_map(axes[0], lons_y, lats_y, tn, title=\"True negative\")\n",
    "        plot_map(axes[1], lons_y, lats_y, fp, title=\"False positive\")\n",
    "        plot_map(axes[2], lons_y, lats_y, fn, title=\"False negative\")\n",
    "        plot_map(axes[3], lons_y, lats_y, tp, title=\"True positive\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    if pred_xtrm:\n",
    "        return np.mean(precision), np.mean(recall), np.mean(roc_auc)\n",
    "    \n",
    "    return np.mean(precision), np.mean(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231ef4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the amount of precipitation\n",
    "df_prec = pd.DataFrame(columns = ['id', 'name', 'opt_model',\n",
    "                                  'train_pr_rmse', 'test_pr_rmse', \n",
    "                                  'train_xtrm_precision', 'test_xtrm_precision', \n",
    "                                  'train_xtrm_recall', 'test_xtrm_recall'])\n",
    "df_xtrm = pd.DataFrame(columns = ['id', 'name', 'opt_model',\n",
    "                                  'train_xtrm_roc_auc', 'test_xtrm_roc_auc', \n",
    "                                  'train_xtrm_precision', 'test_xtrm_precision', \n",
    "                                  'train_xtrm_recall', 'test_xtrm_recall'])\n",
    "\n",
    "apply_for_prec = False\n",
    "apply_for_xtrm = True\n",
    "\n",
    "\n",
    "if apply_for_prec:\n",
    "    print('To be implemented.')\n",
    "    \n",
    "if apply_for_xtrm:\n",
    "    print('To be implemented.')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a495bdc0-fc28-4433-9b28-4e9627a67d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5df59f7-5926-46ac-be5d-f7ca5b71d493",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xtrm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451288bf-8449-4d11-a2c3-de7fb90caa90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
