{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8261233-1afb-4e25-bf5a-d35b8dac588e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= '0.20'\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "#from sklearn.pipeline import Pipelines\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "assert tf.__version__ >= '2.0'\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from keras.models import Sequential\n",
    "#from keras.layers.wrappers import TimeDistributed\n",
    "#from keras.layers import Dense,LSTM,Conv2D, BatchNormalization,Flatten, MaxPooling2D\n",
    "#from keras.layers import Conv2DTranspose,Concatenate,UpSampling2D,Cropping2D\n",
    "#from keras.layers import Input, Lambda, Reshape, Dropout, Activation\n",
    "\n",
    "from tensorflow.keras.layers import Dropout, BatchNormalization, Reshape\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Input, MaxPooling2D, Flatten, MaxPool2D, MaxPool3D, UpSampling2D\n",
    "from tensorflow.keras.layers import Conv2DTranspose, Flatten, Reshape, Cropping2D, Embedding, BatchNormalization,ZeroPadding2D\n",
    "from tensorflow.keras.layers import LeakyReLU, Activation, Input, add, multiply\n",
    "from tensorflow.keras.layers import concatenate, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.layers import Lambda\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import load_model \n",
    "from tensorflow.python.ops import gen_nn_ops\n",
    "\n",
    "#print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# Common imports\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import dask\n",
    "import datetime\n",
    "import math\n",
    "import pathlib\n",
    "import hashlib\n",
    "dask.config.set({'array.slicing.split_large_chunks': False})\n",
    "\n",
    "# To make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# Config matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "# Dotenv\n",
    "from dotenv import dotenv_values\n",
    "# Custom utils\n",
    "from utils.utils_data import *\n",
    "from utils.utils_ml import *\n",
    "from utils.utils_plot import *\n",
    "from utils.utils_unet import *\n",
    "from utils.utils_resnet import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2fd7179-e0f4-46b8-ad91-3ba9a4ac7214",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load test data\n",
    "dg_test_X = np.array(xr.open_dataarray('tmp/data/dg_test_X.nc'))\n",
    "dg_test_Y = np.array(xr.open_dataarray('tmp/data/dg_test_Y.nc'))\n",
    "dg_test_Y_xtrm = np.array(xr.open_dataarray('tmp/data/dg_test_Y_xtrm.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab2c4762-e3d6-4835-bec3-cd3afffb73c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (46, 56, 10)\n",
      "y shape: (46, 56)\n"
     ]
    }
   ],
   "source": [
    "# Define args for the U-net model\n",
    "i_shape = dg_test_X.shape[1:]\n",
    "o_shape = (46,56)\n",
    "\n",
    "print(f'X shape: {i_shape}')\n",
    "print(f'y shape: {o_shape}')\n",
    "output_channels = 1\n",
    "num_filters = 32\n",
    "use_batchnorm = True\n",
    "dropout = True\n",
    "lr = 0.0004\n",
    "#optimizer = tf.optimizers.Adam(learning_rate = lr)\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6116d40c-af62-4a07-b099-24b5d08471c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_mod = ['unet']\n",
    "for imod in names_mod:\n",
    "    tmp_file = pathlib.Path(f'tmp/{imod}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40677186-22d9-43ad-8a93-32262464eb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-19 19:43:41.853095: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-19 19:43:42.568198: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10415 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:0d:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "custom_objects = {\"weighted_cross_entropy_fn\": weighted_binary_cross_entropy}\n",
    "with keras.utils.custom_object_scope(custom_objects):\n",
    "                m = load_model(tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd27f90b-83ed-4282-803d-bab9c76b8b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename(model, layer, new_name):\n",
    "    def _get_node_suffix(name):\n",
    "        for old_name in old_nodes:\n",
    "            if old_name.startswith(name):\n",
    "                return old_name[len(name):]\n",
    "\n",
    "    old_name = layer.name\n",
    "    old_nodes = list(model._network_nodes)\n",
    "    new_nodes = []\n",
    "\n",
    "    for l in model.layers:\n",
    "        if l.name == old_name:\n",
    "            l._name = new_name\n",
    "            # vars(l).__setitem__('_name', new)  # bypasses .__setattr__\n",
    "            new_nodes.append(new_name + _get_node_suffix(old_name))\n",
    "        else:\n",
    "            new_nodes.append(l.name + _get_node_suffix(l.name))\n",
    "    model._network_nodes = set(new_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18d11391-ee75-42a9-b620-a05c5aecb95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename(m, m.layers[len(m.layers)-1], 'predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "407fa107-d1e7-4e16-a223-7ba0ccd61602",
   "metadata": {},
   "outputs": [],
   "source": [
    "image =  dg_test_X[0,:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21bb235d-c030-440a-897c-2ce99ed0929f",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha=2\n",
    "epsilon=1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "638a452b-73d5-4c8c-ad18-1bd310bc0b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework.ops import disable_eager_execution \n",
    "disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00944eb0-cdd3-4dc9-8eef-84559896328f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow.compat.v1 as tf\n",
    "#tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33e061e9-7726-4657-9b2d-5e6ac6f0ca28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelevancePropagation(object):\n",
    "    \"\"\"Very basic implementation of the layer-wise relevance propagation algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, epsilon, input_size, output_size):\n",
    "        self.epsilon = epsilon\n",
    "        # Load model\n",
    "        input_shape = input_size\n",
    "        self.model = model \n",
    "        self.rule = \"z_plus\"\n",
    "        self.pooling_type = \"max\"\n",
    "\n",
    "        # Extract model's weights\n",
    "        self.weights = {weight.name.split('/')[0]: weight for weight in self.model.trainable_weights\n",
    "                        if 'bias' not in weight.name}\n",
    "\n",
    "        # Extract activation layers\n",
    "        self.activations = [layer.output for layer in self.model.layers]\n",
    "        self.activations = self.activations[::-1]\n",
    "\n",
    "        # Extract the model's layers name\n",
    "        self.layer_names = [layer.name for layer in self.model.layers]\n",
    "        self.layer_names = self.layer_names[::-1]\n",
    "\n",
    "        # Build relevance graph\n",
    "        self.relevance = self.relevance_propagation()\n",
    "\n",
    "    def run(self, image):\n",
    "        \"\"\"Computes feature relevance maps for a single image.\n",
    "\n",
    "        Args:\n",
    "            image: array of shape (W, H, C)\n",
    "\n",
    "        Returns:\n",
    "            RGB or grayscale relevance map.\n",
    "\n",
    "        \"\"\"\n",
    "        f = K.function(inputs=self.model.input, outputs=self.relevance)\n",
    "        image = preprocess_input(image)\n",
    "        image = tf.expand_dims(image, axis=0)\n",
    "        relevance_scores = f(inputs=image)\n",
    "        relevance_scores = self.postprocess(relevance_scores)\n",
    "        return np.squeeze(relevance_scores)\n",
    "\n",
    "    def relevance_propagation(self):\n",
    "        \"\"\"Builds graph for relevance propagation.\"\"\"\n",
    "        relevance = self.model.output\n",
    "        print(self.layer_names)\n",
    "        for i, layer_name in enumerate(self.layer_names):\n",
    "            print(i)\n",
    "            print(layer_name)\n",
    "            if 'predictions' in layer_name:\n",
    "                #relevance = self.relprop_dense(self.activations[i+1], self.weights[layer_name], relevance)\n",
    "                relevance = relevance\n",
    "            elif 'fc' in layer_name:\n",
    "                relevance = self.relprop_dense(self.activations[i+1], self.weights[layer_name], relevance)\n",
    "            elif 'flatten' in layer_name:\n",
    "                relevance = self.relprop_flatten(self.activations[i+1], relevance)\n",
    "            elif 'pool' in layer_name:\n",
    "                relevance = self.relprop_pool(self.activations[i+1], relevance)\n",
    "            elif 'conv' in layer_name:\n",
    "                relevance = self.relprop_conv(self.activations[i+1], self.weights[layer_name], relevance, layer_name)\n",
    "            elif 'dropout' in layer_name or 'normalization' in layer_name or 'cropping' in layer_name or 'concatenate' in layer_name or 'activation' in layer_name or 'conv2d_transpose' in layer_name or 'zero_padding2d' in layer_name  :\n",
    "            #elif 'input' in layer_name:\n",
    "                pass\n",
    "            else:\n",
    "                raise Exception(\"Error: layer type not recognized.\")\n",
    "        return relevance\n",
    "\n",
    "    def relprop_dense(self, x, w, r):\n",
    "        \"\"\"Implements relevance propagation rules for dense layers.\n",
    "\n",
    "        Args:\n",
    "            x: array of activations\n",
    "            w: array of weights\n",
    "            r: array of relevance scores\n",
    "\n",
    "        Returns:\n",
    "            array of relevance scores of same dimension as a\n",
    "\n",
    "        \"\"\"\n",
    "        if self.rule == \"z_plus\":\n",
    "            w_pos = tf.maximum(w, 0.0)\n",
    "            z = tf.matmul(x, w_pos) + self.epsilon\n",
    "            s = r / z\n",
    "            c = tf.matmul(s, tf.transpose(w_pos))\n",
    "            return c * x\n",
    "        else:\n",
    "            raise Exception(\"Error: rule for dense layer not implemented.\")\n",
    "\n",
    "    def relprop_flatten(self, x, r):\n",
    "        \"\"\"Transfers relevance scores coming from dense layers to last feature maps of network.\n",
    "\n",
    "        Args:\n",
    "            x: array of activations\n",
    "            r: array of relevance scores\n",
    "\n",
    "        Returns:\n",
    "            array of relevance scores of same dimension as a\n",
    "\n",
    "        \"\"\"\n",
    "        return tf.reshape(r, tf.shape(x))\n",
    "\n",
    "    def relprop_pool(self, x, r, ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1), padding='SAME'):\n",
    "        \"\"\"Implements relevance propagation through pooling layers.\n",
    "\n",
    "        Args:\n",
    "            x: array of activations\n",
    "            r: array of relevance scores\n",
    "            ksize: pooling kernel dimensions used during forward path\n",
    "            strides: step size of pooling kernel used during forward path\n",
    "            padding: parameter for SAME or VALID padding\n",
    "\n",
    "        Returns:\n",
    "            array of relevance scores of same dimensions as a\n",
    "\n",
    "        \"\"\"\n",
    "        if self.pooling_type == \"avg\":\n",
    "            z = tf.nn.avg_pool(x, ksize, strides, padding) + self.epsilon\n",
    "            s = r / z\n",
    "            c = gen_nn_ops.avg_pool_grad(tf.shape(x), s, ksize, strides, padding)\n",
    "        elif self.pooling_type == \"max\":\n",
    "            z = tf.nn.max_pool(x, ksize, strides, padding) + self.epsilon\n",
    "            s = r / z\n",
    "            c = gen_nn_ops.max_pool_grad_v2(x, z, s, ksize, strides, padding)\n",
    "        else:\n",
    "            raise Exception(\"Error: no such unpooling operation implemented.\")\n",
    "        return c * x\n",
    "\n",
    "    def relprop_conv(self, x, w, r, name, strides=(1, 1, 1, 1), padding='SAME'):\n",
    "        \"\"\"Implements relevance propagation rules for convolutional layers.\n",
    "\n",
    "        Args:\n",
    "            x: array of activations\n",
    "            w: array of weights\n",
    "            r: array of relevance scores\n",
    "            name: current layer name\n",
    "            strides: step size of filters used during forward path\n",
    "            padding: parameter for SAME or VALID padding\n",
    "\n",
    "        Returns:\n",
    "            array of relevance scores of same dimensions as a\n",
    "\n",
    "        \"\"\"\n",
    "        if name == 'block1_conv1':\n",
    "            x = tf.ones_like(x)     # only for input\n",
    "\n",
    "        if self.rule == \"z_plus\":\n",
    "            w_pos = tf.maximum(w, 0.0)\n",
    "            z = tf.nn.conv2d(x, w_pos, strides, padding) + self.epsilon\n",
    "            \n",
    "            if (r.shape[1] > z.shape[1]):\n",
    "                r_crop = crop_output(z, r)\n",
    "                s = r_crop / z\n",
    "            else:\n",
    "                z_crop = crop_output(r, z)\n",
    "                s = r / z_crop\n",
    "\n",
    "            #s = r / z\n",
    "            c = tf.compat.v1.nn.conv2d_backprop_input(tf.shape(x), w_pos, s, strides, padding)\n",
    "            return c * x\n",
    "        else:\n",
    "            raise Exception(\"Error: rule for convolutional layer not implemented.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def rescale(x):\n",
    "        \"\"\"Rescales relevance scores of a batch of relevance maps between 0 and 1\n",
    "\n",
    "        Args:\n",
    "            x: RGB or grayscale relevance maps with dimensions (N, W, H, C) or (N, W, H), respectively.\n",
    "\n",
    "        Returns:\n",
    "            Rescaled relevance maps of same dimensions as input\n",
    "\n",
    "        \"\"\"\n",
    "        x_min = np.min(x, axis=(1, 2), keepdims=True)\n",
    "        x_max = np.max(x, axis=(1, 2), keepdims=True)\n",
    "        return (x - x_min).astype(\"float64\") / (x_max - x_min).astype(\"float64\")\n",
    "\n",
    "    def postprocess(self, x):\n",
    "        \"\"\"Postprocesses batch of feature relevance scores (relevance_maps).\n",
    "\n",
    "        Args:\n",
    "            x: array with dimension (N, W, H, C)\n",
    "\n",
    "        Returns:\n",
    "            x: array with dimensions (N, W, H, C) or (N, W, H) depending on if grayscale or not\n",
    "\n",
    "        \"\"\"\n",
    "        if self.grayscale:\n",
    "            x = np.mean(x, axis=-1)\n",
    "        x = self.rescale(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ecdf591-b7e5-429a-9aa5-388b8dba4222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cropping2d_9', 'conv2d_37', 'activation_31', 'conv2d_36', 'dropout_9', 'activation_30', 'conv2d_35', 'concatenate_7', 'cropping2d_8', 'conv2d_transpose_7', 'activation_29', 'conv2d_34', 'dropout_8', 'activation_28', 'conv2d_33', 'concatenate_6', 'cropping2d_7', 'conv2d_transpose_6', 'activation_27', 'conv2d_32', 'dropout_7', 'activation_26', 'conv2d_31', 'concatenate_5', 'cropping2d_6', 'conv2d_transpose_5', 'activation_25', 'conv2d_30', 'dropout_6', 'activation_24', 'conv2d_29', 'concatenate_4', 'cropping2d_5', 'conv2d_transpose_4', 'batch_normalization_3', 'conv2d_28', 'dropout_5', 'batch_normalization_2', 'conv2d_27', 'max_pooling2d_7', 'activation_23', 'conv2d_26', 'spatial_dropout2d_7', 'activation_22', 'conv2d_25', 'max_pooling2d_6', 'activation_21', 'conv2d_24', 'spatial_dropout2d_6', 'activation_20', 'conv2d_23', 'max_pooling2d_5', 'activation_19', 'conv2d_22', 'spatial_dropout2d_5', 'activation_18', 'conv2d_21', 'max_pooling2d_4', 'activation_17', 'conv2d_20', 'spatial_dropout2d_4', 'activation_16', 'conv2d_19', 'zero_padding2d_1', 'input_2']\n",
      "0\n",
      "cropping2d_9\n",
      "1\n",
      "conv2d_37\n",
      "2\n",
      "activation_31\n",
      "3\n",
      "conv2d_36\n",
      "4\n",
      "dropout_9\n",
      "5\n",
      "activation_30\n",
      "6\n",
      "conv2d_35\n",
      "7\n",
      "concatenate_7\n",
      "8\n",
      "cropping2d_8\n",
      "9\n",
      "conv2d_transpose_7\n",
      "10\n",
      "activation_29\n",
      "11\n",
      "conv2d_34\n",
      "12\n",
      "dropout_8\n",
      "13\n",
      "activation_28\n",
      "14\n",
      "conv2d_33\n",
      "15\n",
      "concatenate_6\n",
      "16\n",
      "cropping2d_7\n",
      "17\n",
      "conv2d_transpose_6\n",
      "18\n",
      "activation_27\n",
      "19\n",
      "conv2d_32\n",
      "20\n",
      "dropout_7\n",
      "21\n",
      "activation_26\n",
      "22\n",
      "conv2d_31\n",
      "23\n",
      "concatenate_5\n",
      "24\n",
      "cropping2d_6\n",
      "25\n",
      "conv2d_transpose_5\n",
      "26\n",
      "activation_25\n",
      "27\n",
      "conv2d_30\n",
      "28\n",
      "dropout_6\n",
      "29\n",
      "activation_24\n",
      "30\n",
      "conv2d_29\n",
      "31\n",
      "concatenate_4\n",
      "32\n",
      "cropping2d_5\n",
      "33\n",
      "conv2d_transpose_4\n",
      "34\n",
      "batch_normalization_3\n",
      "35\n",
      "conv2d_28\n",
      "36\n",
      "dropout_5\n",
      "37\n",
      "batch_normalization_2\n",
      "38\n",
      "conv2d_27\n",
      "39\n",
      "max_pooling2d_7\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "You are passing KerasTensor(type_spec=TensorSpec(shape=(None, 6, 8, 512), dtype=tf.float32, name=None), name='activation_23/Relu:0', description=\"created by layer 'activation_23'\"), an intermediate Keras symbolic input/output, to a TF API that does not allow registering custom dispatchers, such as `tf.cond`, `tf.function`, gradient tapes, or `tf.map_fn`. Keras Functional model construction only supports TF API calls that *do* support dispatching, such as `tf.math.add` or `tf.reshape`. Other APIs cannot be called directly on symbolic Kerasinputs/outputs. You can work around this limitation by putting the operation in a custom Keras layer `call` and calling that layer on this symbolic input/output.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/scratch/local/25989280/ipykernel_10830/1331687187.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlrp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRelevancePropagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mo_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/scratch/local/25989280/ipykernel_10830/1231273002.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, epsilon, input_size, output_size)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# Build relevance graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelevance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelevance_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/local/25989280/ipykernel_10830/1231273002.py\u001b[0m in \u001b[0;36mrelevance_propagation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0mrelevance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelprop_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelevance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;34m'pool'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                 \u001b[0mrelevance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelprop_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelevance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;34m'conv'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0mrelevance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelprop_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelevance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/local/25989280/ipykernel_10830/1231273002.py\u001b[0m in \u001b[0;36mrelprop_pool\u001b[0;34m(self, x, r, ksize, strides, padding)\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mksize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_nn_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool_grad_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mksize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error: no such unpooling operation implemented.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mmax_pool_grad_v2\u001b[0;34m(orig_input, orig_output, grad, ksize, strides, padding, data_format, name)\u001b[0m\n\u001b[1;32m   6251\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6252\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6253\u001b[0;31m       return max_pool_grad_v2_eager_fallback(\n\u001b[0m\u001b[1;32m   6254\u001b[0m           \u001b[0morig_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mksize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6255\u001b[0m           data_format=data_format, name=name, ctx=_ctx)\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mmax_pool_grad_v2_eager_fallback\u001b[0;34m(orig_input, orig_output, grad, ksize, strides, padding, data_format, name, ctx)\u001b[0m\n\u001b[1;32m   6283\u001b[0m     \u001b[0mdata_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"NHWC\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6284\u001b[0m   \u001b[0mdata_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data_format\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6285\u001b[0;31m   \u001b[0m_attr_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_inputs_T\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs_to_matching_eager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0morig_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_dtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhalf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6286\u001b[0m   \u001b[0;34m(\u001b[0m\u001b[0morig_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_inputs_T\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6287\u001b[0m   \u001b[0mksize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mksize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36margs_to_matching_eager\u001b[0;34m(l, ctx, allowed_dtypes, default_dtype)\u001b[0m\n\u001b[1;32m    254\u001b[0m       \u001b[0;31m# not list allowed dtypes, in which case we should skip this.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mallowed_dtypes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m         \u001b[0;31m# If we did not match an allowed dtype, try again with the default\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;31m# dtype. This could be because we have an empty tensor and thus we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    341\u001b[0m                                          as_ref=False):\n\u001b[1;32m    342\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0mon\u001b[0m \u001b[0ma\u001b[0m \u001b[0msymbolic\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m   \"\"\"\n\u001b[0;32m--> 267\u001b[0;31m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0m\u001b[1;32m    268\u001b[0m                         allow_broadcast=True)\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    277\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m   \u001b[0;34m\"\"\"Creates a constant on the current device.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/keras/engine/keras_tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m     raise TypeError(\n\u001b[0m\u001b[1;32m    255\u001b[0m         \u001b[0;34mf'You are passing {self}, an intermediate Keras symbolic input/output, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;34m'to a TF API that does not allow registering custom dispatchers, such '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: You are passing KerasTensor(type_spec=TensorSpec(shape=(None, 6, 8, 512), dtype=tf.float32, name=None), name='activation_23/Relu:0', description=\"created by layer 'activation_23'\"), an intermediate Keras symbolic input/output, to a TF API that does not allow registering custom dispatchers, such as `tf.cond`, `tf.function`, gradient tapes, or `tf.map_fn`. Keras Functional model construction only supports TF API calls that *do* support dispatching, such as `tf.math.add` or `tf.reshape`. Other APIs cannot be called directly on symbolic Kerasinputs/outputs. You can work around this limitation by putting the operation in a custom Keras layer `call` and calling that layer on this symbolic input/output."
     ]
    }
   ],
   "source": [
    "lrp = RelevancePropagation(m, epsilon=epsilon, input_size=i_shape, output_size=o_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318c5aa9-4d4c-4015-b875-d25d618a3a52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa985d3-a3dd-4213-b3d8-22ecfe69a48a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5a744917-e208-40cc-9763-95b27d9be0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerwiseRelevancePropagation:\n",
    "\n",
    "    \"\"\"Very basic implementation of the layer-wise relevance propagation algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, epsilon, input_size, output_size):\n",
    "        self.epsilon = epsilon\n",
    "        # Load model\n",
    "        input_shape = input_size\n",
    "        self.model = model \n",
    "        self.pooling_type = \"max\"\n",
    "        self.alpha = alpha\n",
    "        self.beta = 1 - alpha\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.names, self.activations, self.weights = get_model_params(self.model)\n",
    "        self.num_layers = len(self.names)\n",
    "\n",
    "        self.relevance = self.compute_relevances()\n",
    "        #self.lrp_runner = K.function(inputs=[self.model.input, ], outputs=[self.relevance, ])\n",
    "        \n",
    "        \n",
    "    def compute_relevances(self):\n",
    "        r = self.model.output\n",
    "        print(self.num_layers)\n",
    "        for i in range(self.num_layers - 2, -1, -1):\n",
    "            print(i)\n",
    "            # note: max_pooling is not working, need to figure this out\n",
    "            if 'dense' in self.names[i + 1]:\n",
    "                r = backprop_fc(self.weights[i + 1][0], self.weights[i + 1][1], self.activations[i], r)\n",
    "            elif 'flatten' in self.names[i + 1]:\n",
    "                r = backprop_flatten(self.activations[i], r)\n",
    "            elif 'pool' in self.names[i + 1]:\n",
    "                r = backprop_max_pool2d(self.activations[i], r)\n",
    "            elif 'conv2d' in self.names[i + 1]:\n",
    "                r = backprop_conv2d(self.weights[i + 1][0], self.weights[i + 1][1], self.activations[i], r)\n",
    "            #elif 'conv2d_transpose' in names[i + 1]:\n",
    "            #    r = backprop_conv2d(weights[i + 1][0], weights[i + 1][1], activations[i], r, strides=strides[i+1])\n",
    "            elif 'conv1d' in self.names[i + 1]:\n",
    "                r = backprop_conv1d(self.weights[i + 1][0], self.weights[i + 1][1], self.activations[i], r, strides=strides[i+1])\n",
    "            elif 'dropout' in self.names[i + 1] or 'normalization' in self.names[i + 1] or 'cropping' in self.names[i + 1] or 'concatenate' in self.names[i + 1] or 'activation' in self.names[i + 1] or 'conv2d_transpose' in self.names[i + 1] or 'zero_padding2d' in self.names[i + 1]  :\n",
    "            #elif 'input' in layer_name:\n",
    "                pass\n",
    "            else:\n",
    "                raise Exception(\"Error: layer type not recognized.\")\n",
    "           \n",
    "            return r\n",
    "\n",
    "    def backprop_fc(self, w, b, a, r):\n",
    "        w_p = K.maximum(w, 0.)\n",
    "        b_p = K.maximum(b, 0.)\n",
    "        z_p = K.dot(a, w_p) + b_p + self.epsilon\n",
    "        s_p = r / z_p\n",
    "        c_p = K.dot(s_p, K.transpose(w_p))\n",
    "\n",
    "        w_n = K.minimum(w, 0.)\n",
    "        b_n = K.minimum(b, 0.)\n",
    "        z_n = K.dot(a, w_n) + b_n - self.epsilon\n",
    "        s_n = r / z_n\n",
    "        c_n = K.dot(s_n, K.transpose(w_n))\n",
    "\n",
    "        return a * (self.alpha * c_p + self.beta * c_n)\n",
    "\n",
    "    def backprop_flatten(self, a, r):\n",
    "        shape = a.get_shape().as_list()\n",
    "        shape[0] = -1\n",
    "        return K.reshape(r, shape)\n",
    "\n",
    "    def backprop_max_pool2d(self, a, r, ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1)):\n",
    "        z = K.pool2d(a, pool_size=ksize[1:-1], strides=strides[1:-1], padding='valid', pool_mode='max')\n",
    "\n",
    "        z_p = K.maximum(z, 0.) + self.epsilon\n",
    "        s_p = r / z_p\n",
    "        c_p = gen_nn_ops.max_pool_grad_v2(a, z_p, s_p, ksize, strides, padding='VALID')\n",
    "\n",
    "        z_n = K.minimum(z, 0.) - self.epsilon\n",
    "        s_n = r / z_n\n",
    "        c_n = gen_nn_ops.max_pool_grad_v2(a, z_n, s_n, ksize, strides, padding='VALID')\n",
    "\n",
    "        return a * (self.alpha * c_p + self.beta * c_n)\n",
    "\n",
    "    def backprop_conv2d(self, w, b, a, r, strides=(1, 1, 1, 1)):\n",
    "        w_p = K.maximum(w, 0.)\n",
    "        b_p = K.maximum(b, 0.)\n",
    "        z_p = K.conv2d(a, kernel=w_p, strides=strides[1:-1], padding='same') + b_p + self.epsilon\n",
    "        if (r.shape[1] > z_p.shape[1]):\n",
    "            r_p_crop = crop_output(z_p, r)\n",
    "            s_p = r_p_crop / z_p\n",
    "        else:\n",
    "            z_p_crop = crop_output(r, z_p)\n",
    "            s_p = r / z_p_crop\n",
    "        #s_p = r / z_p\n",
    "        c_p = K.tf.nn.conv2d_backprop_input(K.shape(a), w_p, s_p, strides, padding='SAME')\n",
    "\n",
    "        #w_n = K.minimum(w, 0.)\n",
    "        #b_n = K.minimum(b, 0.)\n",
    "        #z_n = K.conv2d(a, kernel=w_n, strides=strides[1:-1], padding='same') + b_n - self.epsilon\n",
    "        #s_n = r / z_n\n",
    "        #c_n = K.tf.nn.conv2d_backprop_input(K.shape(a), w_n, s_n, strides, padding='SAME')\n",
    "\n",
    "        #return a * (self.alpha * c_p + self.beta * c_n)\n",
    "        return  a * c_p\n",
    "\n",
    "    def predict_labels(self, images):\n",
    "        return predict_labels(self.model, images)\n",
    "\n",
    "    def run_lrp(self, images):\n",
    "        print(\"Running LRP on {0} images...\".format(len(images)))\n",
    "        return self.lrp_runner([images, ])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "123f9d41-4a87-4e4a-be1b-37ee3eeb1005",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lrp = RelevancePropagation(model, epsilon=epsilon, input_size=i_shape, output_size=o_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c575a3ed-4740-43f7-a1b7-57dee2b42876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_params(model):\n",
    "    names, activations, weights = [], [], []\n",
    "    for layer in model.layers:\n",
    "        name = layer.name if layer.name != 'predictions' else 'fc_out'\n",
    "        names.append(name)\n",
    "        activations.append(layer.output)\n",
    "        weights.append(layer.get_weights())\n",
    "    return names, activations, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d58d8a41-e103-4a47-a298-56b58169adef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "63\n"
     ]
    }
   ],
   "source": [
    "lrp=LayerwiseRelevancePropagation(m, epsilon=epsilon, input_size=i_shape, output_size=o_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb44731-b0d4-4d8f-85a1-3d933a4ee288",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range( - 2, -1, -1):\n",
    "            print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
