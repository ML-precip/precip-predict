{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d545b75d",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "### Import necessary modules and do some basic setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1427bd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= '0.20'\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "assert tf.__version__ >= '2.0'\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# Common imports\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import dask\n",
    "import datetime\n",
    "import math\n",
    "dask.config.set({'array.slicing.split_large_chunks': False})\n",
    "\n",
    "# To make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# Config matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Dotenv\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "# Custom utils\n",
    "from utils.utils_data import *\n",
    "from utils.utils_ml import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41766d4a",
   "metadata": {},
   "source": [
    "### Define some paths and constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc1d02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dotenv_values(\".env\")\n",
    "\n",
    "# Paths\n",
    "PATH_ERA5 = config['PATH_ERA5']\n",
    "PATH_EOBS = config['PATH_EOBS']\n",
    "\n",
    "# Some constants\n",
    "G = 9.80665 \n",
    "DATE_START = '1979-01-01'\n",
    "DATE_END = '2020-12-31'\n",
    "YY_TRAIN = [1979, 2015]\n",
    "YY_TEST = [2016, 2020]\n",
    "LEVELS = [500, 850, 1000]\n",
    "LONS_INPUT = [-40, 30]\n",
    "LATS_INPUT = [30, 80]\n",
    "LONS_PREC = [-12, 30]\n",
    "LATS_PREC = [32, 72]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c3dbae",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74596028",
   "metadata": {},
   "source": [
    "## Target variable: precipitation field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abeea2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precipitation ERA5\n",
    "pr = get_era5_data(PATH_ERA5 + '/precipitation/day_grid1/*nc', DATE_START, DATE_END, LONS_PREC, LATS_PREC)\n",
    "\n",
    "# Define precipitation extremes using the 95th percentile\n",
    "#pr95 = precip_exceedance_xarray(pr, 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f56ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a dimension to be used as channel in the DNN\n",
    "pr = pr.expand_dims('level', -1)\n",
    "pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2c9f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr.tp[2,:,:].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c847649a",
   "metadata": {},
   "source": [
    "## Input data: meteorological fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce5176d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load geopotential height\n",
    "z = get_era5_data(PATH_ERA5 + '/geopotential/grid1/*.nc',\n",
    "                  DATE_START, DATE_END, LONS_INPUT, LATS_INPUT)\n",
    "z = z.sel(level=LEVELS)\n",
    "\n",
    "# Get Z in geopotential height (m)\n",
    "z.z.values = z.z.values/G\n",
    "\n",
    "# Get axes\n",
    "lats = z.lat\n",
    "lons = z.lon\n",
    "\n",
    "# Load temperature\n",
    "t2m = get_era5_data(PATH_ERA5 + '/temperature/grid1/Grid1_*.nc',\n",
    "                    DATE_START, DATE_END, LONS_INPUT, LATS_INPUT)\n",
    "t2m['time'] = pd.DatetimeIndex(t2m.time.dt.date)\n",
    "t2m = t2m.rename_vars({'T2MMEAN': 't'})\n",
    "\n",
    "# Load relative humidity\n",
    "rh = get_era5_data(PATH_ERA5 + '/relative_humidity/day_grid1/*.nc',\n",
    "                   DATE_START, DATE_END, LONS_INPUT, LATS_INPUT)\n",
    "rh['time'] = pd.DatetimeIndex(rh.time.dt.date)\n",
    "rh = rh.sel(level=LEVELS)\n",
    "\n",
    "# Load wind components\n",
    "u850 = get_era5_data(PATH_ERA5 + '/U_wind/day_grid1/*.nc',\n",
    "                     DATE_START, DATE_END, LONS_INPUT, LATS_INPUT)\n",
    "u850['time'] = pd.DatetimeIndex(u850.time.dt.date)\n",
    "v850 = get_era5_data(PATH_ERA5 + '/V_wind/day_grid1/*.nc',\n",
    "                     DATE_START, DATE_END, LONS_INPUT, LATS_INPUT)\n",
    "v850['time'] = pd.DatetimeIndex(v850.time.dt.date)\n",
    "\n",
    "# Checking dimensions\n",
    "print('dimension of pr:', pr.dims)\n",
    "print('dimension of z', z.dims)\n",
    "print('dimension of t2m:', t2m.dims)\n",
    "print('dimension of rh:', rh.dims)\n",
    "print('dimension of u:', u850.dims)\n",
    "print('dimension of v:', v850.dims)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d54b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge arrays\n",
    "X = xr.merge([z, t2m, rh, u850, v850])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a200bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert lat axis if needed\n",
    "if X.lat[0].values < X.lat[1].values:\n",
    "    X = X.reindex(lat=list(reversed(X.lat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7643ef",
   "metadata": {},
   "source": [
    "### Split data and transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23738ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and test\n",
    "X_train_full = X.sel(time=slice('{}-01-01'.format(YY_TRAIN[0]),\n",
    "                                '{}-12-31'.format(YY_TRAIN[1])))\n",
    "X_test = X.sel(time=slice('{}-01-01'.format(YY_TEST[0]),\n",
    "                          '{}-12-31'.format(YY_TEST[1])))\n",
    "\n",
    "pr_train_full = pr.sel(time=slice('{}-01-01'.format(YY_TRAIN[0]),\n",
    "                                  '{}-12-31'.format(YY_TRAIN[1])))\n",
    "pr_test = pr.sel(time=slice('{}-01-01'.format(YY_TEST[0]),\n",
    "                            '{}-12-31'.format(YY_TEST[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be5e320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data generator\n",
    "dic = {'z': LEVELS,\n",
    "       't': None,\n",
    "       'r': LEVELS,\n",
    "       'u': None,\n",
    "       'v': None}\n",
    "\n",
    "from utils.utils_ml import *\n",
    "\n",
    "YY_VALID = 2005\n",
    "\n",
    "dg_train = WeatherDataGenerator(X_train_full.sel(time=slice(f'{YY_TRAIN[0]}', f'{YY_VALID}')),\n",
    "                                pr_train_full.sel(time=slice(f'{YY_TRAIN[0]}', f'{YY_VALID}')),\n",
    "                                dic, batch_size=64, load=True)\n",
    "dg_valid = WeatherDataGenerator(X_train_full.sel(time=slice(f'{YY_VALID+1}', f'{YY_TRAIN[1]}')),\n",
    "                                pr_train_full.sel(time=slice(f'{YY_VALID+1}', f'{YY_TRAIN[1]}')),\n",
    "                                dic, mean=dg_train.mean, std=dg_train.std,\n",
    "                                batch_size=64, load=True)\n",
    "dg_test = WeatherDataGenerator(X_test, pr_test, dic,\n",
    "                               mean=dg_train.mean, std=dg_train.std,\n",
    "                               batch_size=64, load=True, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429afe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_shape = dg_train.X.shape[1:]\n",
    "o_shape = dg_train.y.shape[1:]\n",
    "\n",
    "print(f'X shape: {i_shape}')\n",
    "print(f'y shape: {o_shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958bdca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.mean(dg_train.y, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc080d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_figs = len(dg_train.X[0,0,0,:])\n",
    "ncols = 5\n",
    "nrows = -(-n_figs // ncols)\n",
    "fig, axes = plt.subplots(figsize=(24, 3*nrows), ncols=ncols, nrows=nrows)\n",
    "for i in range(n_figs):\n",
    "    i_row = i // ncols\n",
    "    i_col = i % ncols\n",
    "    im = axes[i_row, i_col].imshow(np.mean(dg_train.X[:,:,:,i], axis=0))\n",
    "    fig.colorbar(im, ax=axes[i_row, i_col])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ba7bd3",
   "metadata": {},
   "source": [
    "# Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4bb749",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EDMF(tf.keras.Model):\n",
    "    \"\"\"Encoder decoder model factory.\"\"\"\n",
    "\n",
    "    def __init__(self, arch, input_size, output_size, for_extremes=False, latent_dim=128,\n",
    "                 dropout_rate=0.2):\n",
    "        super(EDMF, self).__init__()\n",
    "        self.arch = arch\n",
    "        self.input_size = list(input_size)\n",
    "        self.output_size = list(output_size)\n",
    "        self.for_extremes = for_extremes\n",
    "        self.latent_dim = latent_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.last_activation = 'relu'\n",
    "        if for_extremes:\n",
    "            self.last_activation = 'sigmoid'\n",
    "\n",
    "        if arch == 'Davenport-2021':\n",
    "            self.build_Davenport_2021()\n",
    "        elif arch == 'CNN-v1':\n",
    "            self.build_CNN_v1()\n",
    "        elif arch == 'CNN-v2':\n",
    "            self.build_CNN_v2()\n",
    "        elif arch == 'CNN-v3':\n",
    "            self.build_CNN_v3()\n",
    "        else:\n",
    "            raise('The architecture was not correctly defined')\n",
    "            \n",
    "        self.crop_output()\n",
    "\n",
    "        \n",
    "    def build_Davenport_2021(self):\n",
    "\n",
    "        self.latent_dim = 16\n",
    "        \n",
    "        self.encoder = tf.keras.Sequential(\n",
    "            [\n",
    "                layers.InputLayer(input_shape=self.input_size),\n",
    "                layers.Conv2D(16, 3, padding='same', activity_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "                layers.Activation('relu'),\n",
    "                layers.MaxPooling2D(pool_size=2),\n",
    "                layers.SpatialDropout2D(self.dropout_rate), # In original: simple Dropout\n",
    "                layers.Conv2D(16, 3, padding='same', activity_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "                layers.Activation('relu'),\n",
    "                layers.MaxPooling2D(pool_size=2),\n",
    "                layers.SpatialDropout2D(self.dropout_rate), # In original: simple Dropout\n",
    "                layers.Flatten(),                \n",
    "                layers.Dense(self.latent_dim, activity_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "                layers.Activation('relu')\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        next_size = self.get_size_for(stride_factor=4)\n",
    "\n",
    "        self.decoder = tf.keras.Sequential( # In original: no decoder\n",
    "            [ \n",
    "                layers.InputLayer(input_shape=(self.latent_dim,)),\n",
    "                layers.Dense(np.prod(next_size), activation='relu'),\n",
    "                layers.Reshape(target_shape=next_size),\n",
    "                layers.Conv2DTranspose(16, 3, strides=2, padding='same', activation='relu'),\n",
    "                layers.Conv2DTranspose(16, 3, strides=2, padding='same', activation='relu'),\n",
    "                layers.Conv2DTranspose(1, 3, strides=1, padding='same', activation=self.last_activation),\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def build_CNN_v1(self):\n",
    "        self.encoder = tf.keras.Sequential(\n",
    "            [\n",
    "                layers.InputLayer(input_shape=self.input_size),\n",
    "                layers.Conv2D(32, 3, strides=(2, 2), padding='same', activation='relu'),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.SpatialDropout2D(self.dropout_rate),\n",
    "                layers.Conv2D(64, 3, strides=(2, 2), padding='same', activation='relu'),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.SpatialDropout2D(self.dropout_rate),\n",
    "                layers.Flatten(),\n",
    "                layers.Dense(self.latent_dim, activation='sigmoid'),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        next_size = self.get_size_for(stride_factor=4)\n",
    "\n",
    "        self.decoder = tf.keras.Sequential(\n",
    "            [\n",
    "                layers.InputLayer(input_shape=(self.latent_dim,)),\n",
    "                layers.Dense(np.prod(next_size), activation='relu'),\n",
    "                layers.Reshape(target_shape=next_size),\n",
    "                layers.Conv2DTranspose(64, 3, strides=2, padding='same', activation='relu'),\n",
    "                layers.Conv2DTranspose(32, 3, strides=2, padding='same', activation='relu'),\n",
    "                layers.Conv2DTranspose(1, 3, strides=1, padding='same', activation=self.last_activation),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "    def build_CNN_v2(self):\n",
    "        self.encoder = tf.keras.Sequential(\n",
    "            [\n",
    "                layers.InputLayer(input_shape=self.input_size),\n",
    "                layers.Conv2D(8, 3, padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "                layers.Conv2D(8, 3, strides=(2, 2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.SpatialDropout2D(self.dropout_rate),\n",
    "                layers.Conv2D(16, 3, strides=(2, 2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.SpatialDropout2D(self.dropout_rate),\n",
    "                layers.Conv2D(16, 3, strides=(2, 2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.SpatialDropout2D(self.dropout_rate),\n",
    "                layers.Conv2D(32, 3, strides=(2, 2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.SpatialDropout2D(self.dropout_rate),\n",
    "                layers.Flatten(),\n",
    "                layers.Dense(self.latent_dim, activation=self.last_activation, kernel_initializer='he_normal')\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        next_size = self.get_size_for(stride_factor=16)\n",
    "\n",
    "        self.decoder = tf.keras.Sequential(\n",
    "            [\n",
    "                layers.InputLayer(input_shape=(self.latent_dim,)),\n",
    "                layers.Dense(np.prod(next_size), activation='relu', kernel_initializer='he_normal'),\n",
    "                layers.Reshape(target_shape=next_size),\n",
    "                layers.Conv2DTranspose(16, 3, strides=(2, 2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "                layers.Conv2DTranspose(16, 3, strides=(2, 2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "                layers.Conv2DTranspose(16, 3, strides=(2, 2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "                layers.Conv2DTranspose(8, 3, strides=(2, 2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "                layers.Conv2D(1, 3, strides=(1,1), padding='same', activation='relu', kernel_initializer='he_normal')\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def build_CNN_v3(self):\n",
    "        self.encoder = tf.keras.Sequential(\n",
    "            [\n",
    "                layers.InputLayer(input_shape=self.data_size),\n",
    "                layers.Conv2D(8, 3, padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.SpatialDropout2D(self.dropout_rate),\n",
    "                layers.Conv2D(8, 3, strides=(2,2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.SpatialDropout2D(self.dropout_rate),\n",
    "                layers.Conv2D(16, 3, strides=(2,2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.SpatialDropout2D(self.dropout_rate),\n",
    "                layers.Conv2D(16, 3, strides=(2,2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.SpatialDropout2D(self.dropout_rate),\n",
    "                layers.Conv2D(32, 3, strides=(2,2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.SpatialDropout2D(self.dropout_rate),\n",
    "                layers.Conv2D(32, 3, strides=(2,2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.SpatialDropout2D(self.dropout_rate),\n",
    "                layers.Conv2D(64, 3, strides=(2,2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.SpatialDropout2D(self.dropout_rate),\n",
    "                layers.Flatten(),\n",
    "                layers.Dense(self.latent_dim, activation='sigmoid', kernel_initializer='he_normal')\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        next_size = self.get_size_for(stride_factor=64)\n",
    "\n",
    "        self.decoder = tf.keras.Sequential(\n",
    "            [\n",
    "                layers.InputLayer(input_shape=(self.latent_dim,)),\n",
    "                layers.Dense(np.prod(next_size), activation='relu', kernel_initializer='he_normal'),\n",
    "                layers.Reshape(target_shape=next_size),\n",
    "                layers.Conv2DTranspose(32, 3, strides=(2,2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "                layers.Conv2DTranspose(32, 3, strides=(2,2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "                layers.Conv2DTranspose(16, 3, strides=(2,2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "                layers.Conv2DTranspose(16, 3, strides=(2,2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "                layers.Conv2DTranspose(16, 3, strides=(2,2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "                layers.Conv2DTranspose(8, 3, strides=(2,2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "                layers.Conv2D(1, 3, strides=(1,1), padding='same', activation=self.last_activation, kernel_initializer='he_normal')\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "\n",
    "    def get_size_for(self, stride_factor):\n",
    "        next_shape = self.output_size.copy()\n",
    "        next_shape[0] = int(np.ceil(next_shape[0]/stride_factor))\n",
    "        next_shape[1] = int(np.ceil(next_shape[1]/stride_factor))\n",
    "\n",
    "        return next_shape\n",
    "\n",
    "\n",
    "    def crop_output(self):\n",
    "        # Compute difference between reconstructed width and hight and the desired output size.\n",
    "        h, w = self.decoder.layers[-1].output.get_shape().as_list()[1:3]\n",
    "        h_tgt, w_tgt = self.output_size[:2]\n",
    "        dh = h - h_tgt\n",
    "        dw = w - w_tgt\n",
    "\n",
    "        # Add to decoder cropping layer and final reshaping\n",
    "        self.decoder.add(layers.Cropping2D(cropping=((dh//2, dh-dh//2), (dw//2, dw-dw//2))))\n",
    "        self.decoder.add(layers.Reshape(target_shape=self.output_size,))\n",
    "\n",
    "        \n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7235fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear session and set tf seed\n",
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Early stopping\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10,\n",
    "                                            restore_best_weights=True)\n",
    "                                            \n",
    "# Model hyperparameters\n",
    "hp_model = {'latent_dim': 128,\n",
    "            'dropout_rate': 0.2}\n",
    "\n",
    "# Training hyperparameters\n",
    "hp_training = {'batch_size': 2048,\n",
    "               'epochs': 200,\n",
    "               'callbacks': [callback]}\n",
    "lr = .0004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a18b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['Davenport-2021', 'CNN-v1', 'CNN-v2', 'CNN-v3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231ef4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the amount of precipitation\n",
    "history_pr = []\n",
    "\n",
    "for model in models:\n",
    "    m = EDMF(model, i_shape, o_shape, for_extremes=False, **hp_model)\n",
    "    m.compile(loss='mse', optimizer=tf.optimizers.Adam(learning_rate = lr))\n",
    "\n",
    "    hist = m.fit(dg_train, validation_data=dg_valid, **hp_training)\n",
    "    history_pr.append(hist)\n",
    "\n",
    "    # Plot training evolution\n",
    "    pd.DataFrame(hist.history).plot(figsize=(8, 5))\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3bf75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict precipitation extremes\n",
    "METRICS = [\n",
    "    tf.metrics.CategoricalAccuracy(name='accuracy'),\n",
    "    tf.metrics.Precision(class_id = 1, name='precision'),\n",
    "    tf.metrics.Recall(class_id = 1, name='recall')\n",
    "]\n",
    "\n",
    "history_xtr = []\n",
    "\n",
    "for model in models:\n",
    "    m = EDMF(model, i_shape, o_shape, for_extremes=True, **hp_model)\n",
    "    m.compile(loss=tf.losses.CategoricalCrossentropy(), \n",
    "              optimizer=tf.optimizers.Adam(learning_rate = lr),\n",
    "              metrics=METRICS)\n",
    "\n",
    "    hist = m.fit(dg_train, validation_data=dg_valid, **hp_training)\n",
    "    history_xtr.append(hist)\n",
    "\n",
    "    # Plot training evolution\n",
    "    pd.DataFrame(hist.history).plot(figsize=(8, 5))\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
