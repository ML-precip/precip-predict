{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d545b75d",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "### Import necessary modules and do some basic setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1427bd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= '0.20'\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay, precision_score, recall_score, roc_auc_score, roc_curve\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "assert tf.__version__ >= '2.0'\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# Common imports\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import dask\n",
    "import datetime\n",
    "import math\n",
    "dask.config.set({'array.slicing.split_large_chunks': False})\n",
    "\n",
    "# To make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# Config matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Dotenv\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "# Custom utils\n",
    "from utils.utils_data import *\n",
    "from utils.utils_ml import *\n",
    "from utils.utils_unet import *\n",
    "from utils.utils_resnet import *\n",
    "from utils.utils_plot import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41766d4a",
   "metadata": {},
   "source": [
    "### Define some paths and constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc1d02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dotenv_values(\".env\")\n",
    "\n",
    "# Paths\n",
    "PATH_ERA5 = config['PATH_ERA5']\n",
    "PATH_EOBS = config['PATH_EOBS']\n",
    "\n",
    "# Some constants\n",
    "G = 9.80665\n",
    "\n",
    "# Options\n",
    "DATE_START = '1979-01-01'\n",
    "DATE_END = '2020-12-31'\n",
    "YY_TRAIN = [1979, 2015]\n",
    "YY_TEST = [2016, 2021]\n",
    "LEVELS = [500, 850, 1000]\n",
    "LONS_INPUT = [-25, 30]\n",
    "LATS_INPUT = [30, 75]\n",
    "LONS_PREC = [-25, 30]\n",
    "LATS_PREC = [30, 75]\n",
    "BATCH_SIZE = 64\n",
    "PRECIP_DATA = 'ERA5-low' # Options: ERA5-hi, ERA5-low, E-OBS\n",
    "PRECIP_XTRM = 0.95 # Percentile (threshold) for the extremes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c3dbae",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74596028",
   "metadata": {},
   "source": [
    "## Target variable: precipitation field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abeea2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load precipitation\n",
    "if PRECIP_DATA == 'ERA5-hi':\n",
    "    pr = get_nc_data(PATH_ERA5 + '/precipitation/orig_grid/daily/*nc', DATE_START, DATE_END, LONS_PREC, LATS_PREC)\n",
    "    pr = pr.tp\n",
    "elif PRECIP_DATA == 'ERA5-low':\n",
    "    pr = get_nc_data(PATH_ERA5 + '/precipitation/day_grid1/*nc', DATE_START, DATE_END, LONS_PREC, LATS_PREC)\n",
    "    pr = pr.tp\n",
    "elif PRECIP_DATA in ['E-OBS', 'EOBS']:\n",
    "    pr = get_nc_data(PATH_EOBS + '/*nc', DATE_START, DATE_END, LONS_PREC, LATS_PREC)\n",
    "    pr = pr.rr\n",
    "    pr = pr.coarsen(lon=5, lat=5, boundary=\"trim\").mean()\n",
    "    pr = pr.fillna(0) # Over the seas. Not optimal...\n",
    "else:\n",
    "    raise ValueError('Precipitation data not well defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f56ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a dimension to be used as channel in the DNN\n",
    "pr = pr.expand_dims('level', -1)\n",
    "pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a9308b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the extreme exceedence\n",
    "qq = xr.DataArray(pr).quantile(PRECIP_XTRM, dim='time')\n",
    "pr_xtrm = xr.DataArray(pr > qq)\n",
    "pr_xtrm = pr_xtrm*1 # Transform to number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd208aba-31eb-42f2-a1b0-6d643b2ceadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coordinates for precip\n",
    "lats_y = pr.lat.to_numpy()\n",
    "lons_y = pr.lon.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175c2dbe-bd1d-49a3-b94b-5f949bc04bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the date with the max number of grid points with pr > threshold\n",
    "x = pr_xtrm.sum(dim=['lat', 'lon'])\n",
    "plot_date = x.argmax(dim='time')\n",
    "\n",
    "# Plot precip, threshold exceedance and threshold values\n",
    "fig, axs = plt.subplots(1, 3, figsize=(21,4))\n",
    "\n",
    "plot_map(axs[0], lons_y, lats_y, pr[plot_date,:,:].to_numpy().squeeze(), title=\"Daily precipitation\", cmap=mpl.cm.YlGnBu)\n",
    "plot_map(axs[1], lons_y, lats_y, pr_xtrm[plot_date,:,:].to_numpy().squeeze(), title=\"Threshold exceedance\", cmap=mpl.cm.YlGnBu)\n",
    "plot_map(axs[2], lons_y, lats_y, qq.to_numpy().squeeze(), title=\"Threshold values\", cmap=mpl.cm.YlGnBu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c847649a",
   "metadata": {},
   "source": [
    "## Input data: meteorological fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce5176d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load geopotential height\n",
    "z = get_era5_data(PATH_ERA5 + '/geopotential/grid1/*.nc', DATE_START, DATE_END, LONS_INPUT, LATS_INPUT)\n",
    "z = z.sel(level=LEVELS)\n",
    "\n",
    "# Get Z in geopotential height (m)\n",
    "z.z.values = z.z.values/G\n",
    "\n",
    "# Load temperature\n",
    "t2m = get_era5_data(PATH_ERA5 + '/temperature/grid1/Grid1_*.nc', DATE_START, DATE_END, LONS_INPUT, LATS_INPUT)\n",
    "t2m['time'] = pd.DatetimeIndex(t2m.time.dt.date)\n",
    "t2m = t2m.rename_vars({'T2MMEAN': 't'})\n",
    "\n",
    "# Load relative humidity\n",
    "rh = get_era5_data(PATH_ERA5 + '/relative_humidity/day_grid1/*.nc', DATE_START, DATE_END, LONS_INPUT, LATS_INPUT)\n",
    "rh['time'] = pd.DatetimeIndex(rh.time.dt.date)\n",
    "rh = rh.sel(level=LEVELS)\n",
    "\n",
    "# Load total column water\n",
    "tcw = get_era5_data(PATH_ERA5 + '/total_column_water/day_grid1/*.nc', DATE_START, DATE_END, LONS_INPUT, LATS_INPUT)\n",
    "tcw['time'] = pd.DatetimeIndex(tcw.time.dt.date)\n",
    "\n",
    "# Load wind components\n",
    "u850 = get_era5_data(PATH_ERA5 + '/U_wind/day_grid1/*.nc', DATE_START, DATE_END, LONS_INPUT, LATS_INPUT)\n",
    "u850['time'] = pd.DatetimeIndex(u850.time.dt.date)\n",
    "v850 = get_era5_data(PATH_ERA5 + '/V_wind/day_grid1/*.nc', DATE_START, DATE_END, LONS_INPUT, LATS_INPUT)\n",
    "v850['time'] = pd.DatetimeIndex(v850.time.dt.date)\n",
    "\n",
    "# Checking dimensions\n",
    "print('dimension of pr:', pr.dims)\n",
    "print('dimension of z', z.dims)\n",
    "print('dimension of t2m:', t2m.dims)\n",
    "print('dimension of rh:', rh.dims)\n",
    "print('dimension of tcw:', tcw.dims)\n",
    "print('dimension of u:', u850.dims)\n",
    "print('dimension of v:', v850.dims)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d54b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge arrays\n",
    "X = xr.merge([z, t2m, rh, tcw, u850, v850])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a200bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert lat axis if needed\n",
    "if X.lat[0].values < X.lat[1].values:\n",
    "    X = X.reindex(lat=list(reversed(X.lat)))\n",
    "    \n",
    "# Get axes\n",
    "lats_x = X.lat\n",
    "lons_x = X.lon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7643ef",
   "metadata": {},
   "source": [
    "### Split data and transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23738ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and test\n",
    "X_train_full = X.sel(time=slice(f'{YY_TRAIN[0]}-01-01', f'{YY_TRAIN[1]}-12-31'))\n",
    "X_test = X.sel(time=slice(f'{YY_TEST[0]}-01-01', f'{YY_TEST[1]}-12-31'))\n",
    "\n",
    "pr_train_full = pr.sel(time=slice(f'{YY_TRAIN[0]}-01-01', f'{YY_TRAIN[1]}-12-31'))\n",
    "pr_test = pr.sel(time=slice(f'{YY_TEST[0]}-01-01', f'{YY_TEST[1]}-12-31'))\n",
    "\n",
    "pr_xtrm_train_full = pr_xtrm.sel(time=slice(f'{YY_TRAIN[0]}-01-01', f'{YY_TRAIN[1]}-12-31'))\n",
    "pr_xtrm_test = pr_xtrm.sel(time=slice(f'{YY_TEST[0]}-01-01', f'{YY_TEST[1]}-12-31'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be5e320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data generator\n",
    "dic = {'z': LEVELS,\n",
    "       't': None,\n",
    "       'r': LEVELS,\n",
    "       'tcwv': None,\n",
    "       'u': None,\n",
    "       'v': None}\n",
    "\n",
    "from utils.utils_ml import *\n",
    "\n",
    "YY_VALID = 2005\n",
    "\n",
    "dg_train = DataGeneratorWithExtremes(X_train_full.sel(time=slice(f'{YY_TRAIN[0]}', f'{YY_VALID}')),\n",
    "                                     pr_train_full.sel(time=slice(f'{YY_TRAIN[0]}', f'{YY_VALID}')),\n",
    "                                     pr_xtrm_train_full.sel(time=slice(f'{YY_TRAIN[0]}', f'{YY_VALID}')),\n",
    "                                     dic, batch_size=BATCH_SIZE, load=True)\n",
    "dg_valid = DataGeneratorWithExtremes(X_train_full.sel(time=slice(f'{YY_VALID+1}', f'{YY_TRAIN[1]}')),\n",
    "                                     pr_train_full.sel(time=slice(f'{YY_VALID+1}', f'{YY_TRAIN[1]}')),\n",
    "                                     pr_xtrm_train_full.sel(time=slice(f'{YY_VALID+1}', f'{YY_TRAIN[1]}')),\n",
    "                                     dic, mean=dg_train.mean, std=dg_train.std,\n",
    "                                     batch_size=BATCH_SIZE, load=True)\n",
    "dg_test = DataGeneratorWithExtremes(X_test, pr_test, pr_xtrm_test, dic,\n",
    "                                    mean=dg_train.mean, std=dg_train.std,\n",
    "                                    batch_size=BATCH_SIZE, load=True, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429afe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_shape = dg_train.X.shape[1:]\n",
    "o_shape = dg_train.y.shape[1:]\n",
    "\n",
    "print(f'X shape: {i_shape}')\n",
    "print(f'y shape: {o_shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8d1475-9b93-4ed7-8fd8-a78894d7e62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the mean precipitation\n",
    "fig = plt.figure(figsize=(7,5))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "vals = np.mean(dg_train.y, axis=0).squeeze()\n",
    "plot_map(ax, lons_y, lats_y, vals, title=\"Mean precipitation\", cmap=mpl.cm.YlGnBu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb29c12-e9c3-4a97-b0c1-88ad4ff1de7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the mean of the predictors\n",
    "n_figs = len(dg_train.X[0,0,0,:])\n",
    "ncols = 5\n",
    "nrows = -(-n_figs // ncols)\n",
    "fig, axes = plt.subplots(figsize=(24, 3.2*nrows), ncols=ncols, nrows=nrows)\n",
    "for i in range(n_figs):\n",
    "    i_row = i // ncols\n",
    "    i_col = i % ncols\n",
    "    ax = axes[i_row, i_col]\n",
    "    vals = np.mean(dg_train.X[:,:,:,i], axis=0).to_numpy()\n",
    "    plot_map(ax, lons_x, lats_x, vals, title=f\"Average of feature {i+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ba7bd3",
   "metadata": {},
   "source": [
    "# Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4bb749",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepFactory(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Model factory.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, arch, input_size, output_size, for_extremes=False, latent_dim=128, dropout_rate=0.2, \n",
    "                 use_batch_norm=True, inner_activation='relu', output_scaling=1, output_crop=None):\n",
    "        super(DeepFactory, self).__init__()\n",
    "        self.arch = arch\n",
    "        self.input_size = list(input_size)\n",
    "        self.output_size = list(output_size)\n",
    "        self.for_extremes = for_extremes\n",
    "        self.latent_dim = latent_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        self.inner_activation = inner_activation\n",
    "        self.output_scaling = output_scaling\n",
    "        self.output_crop = output_crop\n",
    "        \n",
    "        self.last_activation = 'relu'\n",
    "        if for_extremes:\n",
    "            self.last_activation = 'sigmoid'\n",
    "\n",
    "        if arch == 'Davenport-2021':\n",
    "            self.build_Davenport_2021()\n",
    "        elif arch == 'CNN-2L':\n",
    "            self.build_CNN_2L()\n",
    "        elif arch == 'Unet':\n",
    "            self.build_Unet()\n",
    "        elif arch == 'Pan-2019':\n",
    "            self.build_Pan_2019()\n",
    "        elif arch =='Conv-LTSM':\n",
    "            self.build_convLTSM()\n",
    "        else:\n",
    "            raise ValueError('The architecture was not correctly defined')\n",
    "        \n",
    "        \n",
    "    def build_Davenport_2021(self):\n",
    "        \"\"\"\n",
    "        Based on: Davenport, F. V., & Diffenbaugh, N. S. (2021). Using Machine Learning to \n",
    "        Analyze Physical Causes of Climate Change: A Case Study of U.S. Midwest Extreme Precipitation. \n",
    "        Geophysical Research Letters, 48(15). https://doi.org/10.1029/2021GL093787\n",
    "        \"\"\"\n",
    "        \n",
    "        # Downsampling\n",
    "        inputs = layers.Input(shape=self.input_size)\n",
    "        x = layers.Conv2D(16, 3, padding='same', activity_regularizer=regularizers.l2(0.01))(inputs)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "        x = layers.SpatialDropout2D(self.dropout_rate)(x) # In original: simple Dropout\n",
    "        x = layers.Conv2D(16, 3, padding='same', activity_regularizer=regularizers.l2(0.01))(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "        x = layers.SpatialDropout2D(self.dropout_rate)(x) # In original: simple Dropout\n",
    "        x = layers.Flatten()(x)                \n",
    "        x = layers.Dense(self.latent_dim, activity_regularizer=regularizers.l2(0.001))(x) # In original: 16\n",
    "        x = layers.Activation('relu')(x)\n",
    "\n",
    "        next_shape = self.get_shape_for(stride_factor=4)\n",
    "\n",
    "        # Upsampling. In original: no decoder\n",
    "        x = self.dense_block(x, np.prod(next_shape))\n",
    "        x = layers.Reshape(target_shape=next_shape)(x)\n",
    "        x = self.deconv_block(x, 16, 3, stride=2)\n",
    "        x = self.deconv_block(x, 16, 3, stride=2)\n",
    "        x = self.conv_block(x, 1, 3, activation=self.last_activation)\n",
    "        outputs = self.final_cropping_block(x)\n",
    " \n",
    "        self.model = keras.Model(inputs, outputs, name=\"Davenport-2021\")\n",
    "        \n",
    "        \n",
    "    def build_Pan_2019(self):\n",
    "        \"\"\"\n",
    "        Based on: Pan, B., Hsu, K., AghaKouchak, A., & Sorooshian, S. (2019). \n",
    "        Improving Precipitation Estimation Using Convolutional Neural Network. \n",
    "        Water Resources Research, 55(3), 2301–2321. https://doi.org/10.1029/2018WR024090\n",
    "        \"\"\"\n",
    "        # In original: padding='valid'\n",
    "        \n",
    "        # Downsampling\n",
    "        inputs = layers.Input(shape=self.input_size)\n",
    "        x = layers.Conv2D(15, 4, padding='same')(inputs)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Conv2D(20, 4, padding='same')(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "        x = layers.Conv2D(20, 4, padding='same')(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "        x = layers.Flatten()(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "        x = layers.Dense(self.latent_dim, activation='relu')(x) # In original: 60\n",
    "\n",
    "        next_shape = self.get_shape_for(stride_factor=4)\n",
    "\n",
    "        # Upsampling. In original: no decoder\n",
    "        x = self.dense_block(x, np.prod(next_shape))\n",
    "        x = layers.Reshape(target_shape=next_shape)(x)\n",
    "        x = self.deconv_block(x, 20, 4, stride=2)\n",
    "        x = self.deconv_block(x, 20, 4, stride=2)\n",
    "        x = self.conv_block(x, 15, 4)\n",
    "        x = self.conv_block(x, 1, 3, activation=self.last_activation)\n",
    "        outputs = self.final_cropping_block(x)\n",
    "\n",
    "        self.model = keras.Model(inputs, outputs, name=\"Pan-2019\")\n",
    "\n",
    "\n",
    "    def build_CNN_2L(self):\n",
    "\n",
    "        # Downsampling\n",
    "        inputs = layers.Input(shape=self.input_size)\n",
    "        x = self.conv_block(inputs, 32, 3, stride=2, with_batchnorm=True, with_dropout=True)\n",
    "        x = self.conv_block(x, 64, 3, stride=2, with_batchnorm=True, with_dropout=True)\n",
    "        x = layers.Flatten()(x)\n",
    "        x = self.dense_block(x, self.latent_dim, activation='sigmoid')\n",
    "\n",
    "        next_shape = self.get_shape_for(stride_factor=4)\n",
    "\n",
    "        # Upsampling\n",
    "        x = self.dense_block(x, np.prod(next_shape))\n",
    "        x = layers.Reshape(target_shape=next_shape)(x)\n",
    "        x = self.deconv_block(x, 64, 3, stride=2)\n",
    "        x = self.deconv_block(x, 32, 3, stride=2)\n",
    "        x = self.deconv_block(x, 1, 3, activation=self.last_activation)\n",
    "        outputs = self.final_cropping_block(x)\n",
    "\n",
    "        self.model = keras.Model(inputs, outputs, name=\"CNN-v1\")\n",
    "        \n",
    "\n",
    "    def build_convLTSM(self):\n",
    "        \n",
    "        \n",
    "        \n",
    "        inputs = layers.Input(shape=self.input_size)\n",
    "        # the order of the extended_shape must be after the input layer\n",
    "        expanded_shape = self.input_size\n",
    "        expanded_shape.insert(0, 1)\n",
    "        \n",
    "        x = layers.Reshape(target_shape=expanded_shape)(inputs)\n",
    "        \n",
    "        x = layers.ConvLSTM2D(filters=32, kernel_size=(3, 3),\n",
    "                                         return_sequences=True,\n",
    "                                         padding = 'same',\n",
    "                                         go_backwards=True,\n",
    "                                         activation='tanh', recurrent_activation='hard_sigmoid',\n",
    "                                         data_format = 'channels_last',\n",
    "                                         kernel_initializer='glorot_uniform', unit_forget_bias=True, \n",
    "                                         dropout=0.4, recurrent_dropout=0.2\n",
    "                                         )(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.ConvLSTM2D(filters=16, kernel_size=(3, 3),\n",
    "                                         return_sequences=True,\n",
    "                                         go_backwards=True,\n",
    "                                         padding = 'same',\n",
    "                                         activation='tanh', recurrent_activation='hard_sigmoid',\n",
    "                                         kernel_initializer='glorot_uniform', unit_forget_bias=True, \n",
    "                                         data_format = 'channels_last',\n",
    "                                         dropout=0.4, recurrent_dropout=0.2\n",
    "                                         )(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.ConvLSTM2D(filters=8, kernel_size=(3, 3),\n",
    "                                         return_sequences=False,\n",
    "                                         go_backwards=True,\n",
    "                                         padding = 'same',\n",
    "                                         activation='tanh', recurrent_activation='hard_sigmoid',\n",
    "                                         kernel_initializer='glorot_uniform', unit_forget_bias=True, \n",
    "                                         data_format = 'channels_last',\n",
    "                                         dropout=0.3, recurrent_dropout=0.2\n",
    "                                         )(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Conv2D(filters=16, kernel_size=(1, 1),\n",
    "                           activation='relu',\n",
    "                           data_format='channels_last')(x)\n",
    "\n",
    "\n",
    "        x = layers.MaxPooling2D(pool_size=(2, 2), padding='same')(x)\n",
    "        #model.add(Flatten())\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.25)(x)\n",
    "\n",
    "        x = layers.Conv2DTranspose(filters=1, kernel_size=(1, 1), strides=(2, 2), \n",
    "                           activation='sigmoid',padding='same',\n",
    "                           data_format='channels_last')(x)\n",
    "\n",
    "        outputs = self.final_cropping_block(x)\n",
    "\n",
    "        self.model = keras.Model(inputs, outputs, name=\"Conv-LTSM\")\n",
    "        \n",
    "        \n",
    "    def build_Unet(self):\n",
    "        \"\"\"\n",
    "        Based on: U-Net: https://github.com/nikhilroxtomar/Unet-for-Person-Segmentation/blob/main/model.py\n",
    "        \"\"\"\n",
    "        \n",
    "        filters_nb = 64\n",
    "        \n",
    "        # Downsampling\n",
    "        inputs = layers.Input(shape=self.input_size)\n",
    "        \n",
    "        # Pad if necessary\n",
    "        x = self.padding_block(inputs, factor=16)\n",
    "        \n",
    "        s1, p1 = self.unet_encoder_block(x, filters_nb)\n",
    "        s2, p2 = self.unet_encoder_block(p1, filters_nb * 2)\n",
    "        s3, p3 = self.unet_encoder_block(p2, filters_nb * 4)\n",
    "        s4, p4 = self.unet_encoder_block(p3, filters_nb * 8)\n",
    "        \n",
    "        x = self.conv_block(p4, filters_nb * 16, 3, initializer='he_normal', with_batchnorm=True, with_dropout=True)\n",
    "        b1 = self.conv_block(x, filters_nb * 16, 3, initializer='he_normal', with_batchnorm=True)\n",
    "\n",
    "        # Upsampling\n",
    "        d1 = self.unet_decoder_block(b1, s4, filters_nb * 8)\n",
    "        d2 = self.unet_decoder_block(d1, s3, filters_nb * 4)\n",
    "        d3 = self.unet_decoder_block(d2, s2, filters_nb * 2)\n",
    "        d4 = self.unet_decoder_block(d3, s1, filters_nb, is_last=True)\n",
    "        \n",
    "        # Additional upsampling for downscaling\n",
    "        x = self.handle_output_scaling(d4)\n",
    "\n",
    "        x = self.conv_block(x, 1, 1, activation=self.last_activation)\n",
    "        outputs = self.final_cropping_block(x)\n",
    "\n",
    "        self.model = keras.Model(inputs, outputs, name=\"U-Net-v1\")\n",
    "        \n",
    "        \n",
    "    def unet_encoder_block(self, input, filters, kernel_size=3):\n",
    "        x = self.conv_block(input, filters, kernel_size, initializer='he_normal', with_batchnorm=True, with_dropout=True)\n",
    "        x = self.conv_block(x, filters, kernel_size, initializer='he_normal', with_batchnorm=True)\n",
    "        p = layers.MaxPooling2D((2, 2))(x)\n",
    "        \n",
    "        return x, p\n",
    "\n",
    "    \n",
    "    def unet_decoder_block(self, input, skip_features, filters, conv_kernel_size=3, deconv_kernel_size=2, is_last=False):\n",
    "        x = self.deconv_block(input, filters, deconv_kernel_size, stride=2)\n",
    "        x = layers.Concatenate()([x, skip_features])\n",
    "        x = self.conv_block(x, filters, conv_kernel_size, initializer='he_normal', with_batchnorm=True, with_dropout=True)\n",
    "        x = self.conv_block(x, filters, conv_kernel_size, initializer='he_normal', with_batchnorm=(not is_last))\n",
    "\n",
    "        return x\n",
    "        \n",
    "        \n",
    "    def conv_block(self, input, filters, kernel_size=3, stride=1, padding='same', initializer='default', activation='default', \n",
    "                   with_batchnorm=False, with_pooling=False, with_dropout=False, with_late_activation=False):\n",
    "        if activation == 'default':\n",
    "            activation = self.inner_activation\n",
    "            \n",
    "        conv_activation = activation\n",
    "        if with_late_activation:\n",
    "            conv_activation = None\n",
    "            \n",
    "        if initializer == 'default':\n",
    "            x = layers.Conv2D(filters, kernel_size, strides=(stride, stride), padding=padding, activation=conv_activation)(input)\n",
    "        else:\n",
    "            x = layers.Conv2D(filters, kernel_size, strides=(stride, stride), padding=padding, activation=conv_activation, kernel_initializer=initializer)(input)\n",
    "            \n",
    "        if with_batchnorm:\n",
    "            x = layers.BatchNormalization()(x)\n",
    "        if with_late_activation:\n",
    "            x = layers.Activation(activation)(x)\n",
    "        if with_pooling:\n",
    "            x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "        if with_dropout:\n",
    "            x = layers.SpatialDropout2D(self.dropout_rate)(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def deconv_block(self, input, filters, kernel_size=3, stride=1, padding='same', initializer='default', activation='default', \n",
    "                     with_batchnorm=False, with_dropout=False):\n",
    "        if activation == 'default':\n",
    "            activation = self.inner_activation\n",
    "        \n",
    "        if initializer == 'default':\n",
    "            x = layers.Conv2DTranspose(filters, kernel_size, strides=stride, padding=padding, activation=activation)(input)\n",
    "        else:\n",
    "            x = layers.Conv2DTranspose(filters, kernel_size, strides=stride, padding=padding, activation=activation, kernel_initializer=initializer)(input)\n",
    "            \n",
    "        if with_batchnorm:\n",
    "            x = layers.BatchNormalization()(x)\n",
    "        if with_dropout:\n",
    "            x = layers.SpatialDropout2D(self.dropout_rate)(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def dense_block(self, input, units, activation='default', with_dropout=False):\n",
    "        if activation == 'default':\n",
    "            activation=self.inner_activation\n",
    "            \n",
    "        x = layers.Dense(units, activation=activation)(input)\n",
    "        if with_dropout:\n",
    "            x = layers.Dropout(self.dropout_rate)(x)\n",
    "            \n",
    "        return x\n",
    "\n",
    "    \n",
    "    def handle_output_scaling(self, input, with_batchnorm=False):\n",
    "        if self.output_scaling > 1:\n",
    "            if self.output_scaling == 2:\n",
    "                x = self.deconv_block(input, 64, 3, stride=2, with_batchnorm=with_batchnorm)\n",
    "            elif self.output_scaling == 3:\n",
    "                x = self.deconv_block(input, 64, 3, stride=3, with_batchnorm=with_batchnorm)\n",
    "            elif self.output_scaling == 4:\n",
    "                x = self.deconv_block(input, 64, 3, stride=2, with_batchnorm=with_batchnorm)\n",
    "                x = self.deconv_block(x, 64, 3, stride=2, with_batchnorm=with_batchnorm)\n",
    "            elif self.output_scaling == 5:\n",
    "                x = self.deconv_block(input, 64, 3, stride=3, with_batchnorm=with_batchnorm)\n",
    "                x = self.deconv_block(x, 64, 3, stride=2, with_batchnorm=with_batchnorm)\n",
    "            else:\n",
    "                raise NotImplementedError('Level of downscaling not implemented')\n",
    "        else:\n",
    "            x = input\n",
    "        \n",
    "        if self.output_crop:\n",
    "            raise NotImplementedError('Manual cropping not yet implemented')\n",
    "            \n",
    "        return x\n",
    "            \n",
    "        \n",
    "    def padding_block(self, x, factor):\n",
    "        h, w = x.get_shape().as_list()[1:3]\n",
    "        dh = 0\n",
    "        dw = 0\n",
    "        if h % factor > 0:\n",
    "            dh = factor - h % factor\n",
    "        if w % factor > 0:\n",
    "            dw = factor - w % factor\n",
    "        if dh > 0 or dw > 0:\n",
    "            top_pad = dh//2\n",
    "            bottom_pad = dh//2 + dh%2\n",
    "            left_pad = dw//2\n",
    "            right_pad = dw//2 + dw%2\n",
    "            x = layers.ZeroPadding2D(padding=((top_pad, bottom_pad), (left_pad, right_pad)))(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        \n",
    "    def final_cropping_block(self, x):\n",
    "        # Compute difference between reconstructed width and hight and the desired output size.\n",
    "        h, w = x.get_shape().as_list()[1:3]\n",
    "        h_tgt, w_tgt = self.output_size[:2]\n",
    "        dh = h - h_tgt\n",
    "        dw = w - w_tgt\n",
    "\n",
    "        if dh < 0 or dw < 0:\n",
    "            raise ValueError(f'Negative values in output cropping dh={dh} and dw={dw}')\n",
    "\n",
    "        # Add to decoder cropping layer and final reshaping\n",
    "        x = layers.Cropping2D(cropping=((dh//2, dh-dh//2), (dw//2, dw-dw//2)))(x)\n",
    "        #x = layers.Reshape(target_shape=self.output_size,)(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "\n",
    "    def get_shape_for(self, stride_factor):\n",
    "        next_shape = self.output_size.copy()\n",
    "        next_shape[0] = int(np.ceil(next_shape[0]/stride_factor))\n",
    "        next_shape[1] = int(np.ceil(next_shape[1]/stride_factor))\n",
    "\n",
    "        return next_shape\n",
    "\n",
    "        \n",
    "    def call(self, x):\n",
    "        return self.model(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfdc976-5d2f-4c0d-9bb7-bff0b633ef26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the necessary output scaling.\n",
    "dlons_x = float(lons_x[1] - lons_x[0])\n",
    "dlats_x = float(lats_x[0] - lats_x[1])\n",
    "dlons_y = float(lons_y[1] - lons_y[0])\n",
    "dlats_y = float(lats_y[0] - lats_y[1])\n",
    "\n",
    "output_scaling = int(dlons_x / dlons_y)\n",
    "output_crop = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edec5d6b-38f3-49f1-9d9e-25b3116d791c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiate_optimizer(lr_method, lr=.0004, init_lr=0.01, max_lr=0.01):\n",
    "    if lr_method == 'Cyclical':\n",
    "        # Cyclical learning rate\n",
    "        steps_per_epoch = dg_train.n_samples // BATCH_SIZE\n",
    "        clr = tfa.optimizers.CyclicalLearningRate(\n",
    "            initial_learning_rate=init_lr,\n",
    "            maximal_learning_rate=max_lr,\n",
    "            scale_fn=lambda x: 1/(2.**(x-1)),\n",
    "            step_size=2 * steps_per_epoch)\n",
    "        optimizer = tf.keras.optimizers.Adam(clr)\n",
    "    elif lr_method == 'CosineDecay':\n",
    "        decay_steps = EPOCHS * (dg_train.n_samples / BATCH_SIZE)\n",
    "        lr_decayed_fn = tf.keras.optimizers.schedules.CosineDecay(\n",
    "            init_lr, decay_steps)\n",
    "        optimizer = tf.keras.optimizers.Adam(lr_decayed_fn)\n",
    "    elif lr_method == 'Constant':\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate = lr)\n",
    "    else:\n",
    "        raise ValueError('learning rate schedule not well defined.')\n",
    "        \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e21809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute weights for the weighted binary crossentropy\n",
    "weights = class_weight.compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(pr_xtrm.values),\n",
    "    y=pr_xtrm.values.flatten()\n",
    ")\n",
    "\n",
    "print('Weights for the weighted binary crossentropy:')\n",
    "print(f'Classes: {np.unique(pr_xtrm.values)}, weights: {weights}')\n",
    "\n",
    "# Create loss function for the extremes\n",
    "xtrm_loss = weighted_binary_cross_entropy(\n",
    "    weights={0: weights[0].astype('float32'), 1: weights[1].astype('float32')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a7ed93-3060-473b-ad44-a0e45b3eda77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_predictions(y_pred, dg, qq, pred_xtrm=False, show_plots=True, plot_most_extreme=True, plot_worst_best=False, plot_scores=True, plot_confusion_components=True):\n",
    "    if pred_xtrm:\n",
    "        y_pred_bool = y_pred >= 0.5\n",
    "    else:\n",
    "        y_pred_bool = y_pred > qq.to_numpy().squeeze()\n",
    "        \n",
    "    # Multiply to transorm to numeric values\n",
    "    y_pred_bool = y_pred_bool * 1\n",
    "    \n",
    "    # Extract true values\n",
    "    y_xtrm = dg.y_xtrm.to_numpy().squeeze()\n",
    "    y_prec = dg.y.to_numpy().squeeze()\n",
    "    \n",
    "    # Get the index of the max # of extremes\n",
    "    i_max_obs = np.argmax(np.sum(y_xtrm, axis=(1,2)))\n",
    "    \n",
    "    if show_plots and plot_most_extreme:\n",
    "        if pred_xtrm:\n",
    "            fig, axes = plt.subplots(figsize=(12, 3.5), ncols=2, nrows=1)\n",
    "            plot_map(axes[0], lons_y, lats_y, y_xtrm[i_max_obs], title=\"Day with max # extremes - truth (xtrm)\", vmin=0, vmax=1)\n",
    "            plot_map(axes[1], lons_y, lats_y, y_pred[i_max_obs], title=\"Day with max # extremes - prediction (prob xtrm)\", vmin=0, vmax=1)\n",
    "        else:\n",
    "            fig, axes = plt.subplots(figsize=(24, 3.5), ncols=4, nrows=1)\n",
    "            vmax = max(np.max(y_prec[i_max_obs]), np.max(y_pred[i_max_obs]))\n",
    "            plot_map(axes[0], lons_y, lats_y, y_prec[i_max_obs], title=\"Day with max # extremes - truth (val)\", vmin=0, vmax=vmax)\n",
    "            plot_map(axes[1], lons_y, lats_y, y_pred[i_max_obs], title=\"Day with max # extremes - prediction (val)\", vmin=0, vmax=vmax)\n",
    "            plot_map(axes[2], lons_y, lats_y, y_xtrm[i_max_obs], title=\"Day with max # extremes - truth (xtrm)\", vmin=0, vmax=1)\n",
    "            plot_map(axes[3], lons_y, lats_y, y_pred_bool[i_max_obs], title=\"Day with max # extremes - prediction (xtrm)\", vmin=0, vmax=1)\n",
    "        \n",
    "    # Get the index of the max/min difference between prediction and obs\n",
    "    y_diffs_series = np.sum(np.absolute(y_xtrm - y_pred_bool), axis=(1,2))\n",
    "    i_worst_pred = np.argmax(y_diffs_series)\n",
    "    i_best_pred = np.argmin(y_diffs_series)\n",
    "    \n",
    "    if show_plots and plot_worst_best:\n",
    "        if pred_xtrm:\n",
    "            fig, axes = plt.subplots(figsize=(24, 3.5), ncols=4, nrows=1)\n",
    "            plot_map(axes[0], lons_y, lats_y, y_xtrm[i_worst_pred], title=\"Worst prediction - truth\", vmin=0, vmax=1)\n",
    "            plot_map(axes[1], lons_y, lats_y, y_pred[i_worst_pred], title=\"Worst prediction - prediction\", vmin=0, vmax=1)\n",
    "            plot_map(axes[2], lons_y, lats_y, y_xtrm[i_best_pred], title=\"Best prediction - truth\", vmin=0, vmax=1)\n",
    "            plot_map(axes[3], lons_y, lats_y, y_pred[i_best_pred], title=\"Best prediction - prediction\", vmin=0, vmax=1)\n",
    "        else:\n",
    "            fig, axes = plt.subplots(figsize=(24, 3.5), ncols=4, nrows=1)\n",
    "            plot_map(axes[0], lons_y, lats_y, y_xtrm[i_worst_pred], title=\"Worst prediction - truth\", vmin=0, vmax=1)\n",
    "            plot_map(axes[1], lons_y, lats_y, y_pred_bool[i_worst_pred], title=\"Worst prediction - prediction\", vmin=0, vmax=1)\n",
    "            plot_map(axes[2], lons_y, lats_y, y_xtrm[i_best_pred], title=\"Best prediction - truth\", vmin=0, vmax=1)\n",
    "            plot_map(axes[3], lons_y, lats_y, y_pred_bool[i_best_pred], title=\"Best prediction - prediction\", vmin=0, vmax=1)\n",
    "    \n",
    "    # Compute scores\n",
    "    precision, recall = eval_confusion_matrix_scores_on_map(y_xtrm, y_pred_bool)\n",
    "\n",
    "    if pred_xtrm:\n",
    "        roc_auc = eval_roc_auc_score_on_map(y_xtrm, y_pred)\n",
    "        \n",
    "    if show_plots and plot_scores:\n",
    "        if pred_xtrm:\n",
    "            fig, axes = plt.subplots(figsize=(18, 4), ncols=3, nrows=1)\n",
    "        else:\n",
    "            fig, axes = plt.subplots(figsize=(12, 4), ncols=2, nrows=1)\n",
    "\n",
    "        plot_map(axes[0], lons_y, lats_y, precision, title=\"Precision\", vmin=0, vmax=1)\n",
    "        plot_map(axes[1], lons_y, lats_y, recall, title=\"Recall\", vmin=0, vmax=1)\n",
    "        if pred_xtrm:\n",
    "            plot_map(axes[2], lons_y, lats_y, roc_auc, title=\"ROC AUC\", vmin=0.5, vmax=1)\n",
    "\n",
    "    if show_plots and plot_confusion_components:\n",
    "        tn, fp, fn, tp = eval_confusion_matrix_on_map(y_xtrm, y_pred_bool)\n",
    "        fig, axes = plt.subplots(figsize=(24, 4), ncols=4, nrows=1)\n",
    "        plot_map(axes[0], lons_y, lats_y, tn, title=\"True negative\")\n",
    "        plot_map(axes[1], lons_y, lats_y, fp, title=\"False positive\")\n",
    "        plot_map(axes[2], lons_y, lats_y, fn, title=\"False negative\")\n",
    "        plot_map(axes[3], lons_y, lats_y, tp, title=\"True positive\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    if pred_xtrm:\n",
    "        return np.mean(precision), np.mean(recall), np.mean(roc_auc)\n",
    "    \n",
    "    return np.mean(precision), np.mean(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0ee543-418c-4371-a4c5-548e71077fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "EPOCHS = 100\n",
    "LR_METHOD = 'Constant'  # Cyclical, CosineDecay, Constant\n",
    "    \n",
    "# Early stopping\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10,\n",
    "                                            restore_best_weights=True)\n",
    "                                            \n",
    "# Default model options\n",
    "opt_model = {'latent_dim': 128,\n",
    "             'dropout_rate': 0.2}\n",
    "\n",
    "# Default training options\n",
    "opt_training = {'epochs': EPOCHS,\n",
    "                'callbacks': [callback]}\n",
    "\n",
    "# Default optimizer options\n",
    "opt_optimizer = {'lr_method': 'Constant',\n",
    "                 'lr': 0.0004,\n",
    "                 'init_lr': 0.01}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a743731-9946-4acf-b3ba-04f8318256c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "          'Dav-orig': {'model': 'Davenport-2021', 'run': False,\n",
    "                       'opt_model': {'latent_dim': 16},\n",
    "                       'opt_optimizer': {'lr_method': 'Constant'}}, # original\n",
    "          'Dav-64': {'model': 'Davenport-2021', 'run': False,\n",
    "                     'opt_model': {'latent_dim': 64},\n",
    "                     'opt_optimizer': {'lr_method': 'Constant'}},\n",
    "          'CNN-2l': {'model': 'CNN-2L', 'run': False,\n",
    "                     'opt_model': {'latent_dim': 64},\n",
    "                     'opt_optimizer': {'lr_method': 'Constant'}},\n",
    "          'UNET': {'model': 'Unet', 'run': False,\n",
    "                   'opt_model': {'output_scaling': output_scaling, 'output_crop': output_crop},\n",
    "                   'opt_optimizer': {'lr_method': 'CosineDecay'}},\n",
    "          'Pan-orig': {'model': 'Pan-2019', 'run': False,\n",
    "                       'opt_model': {'latent_dim': 60},\n",
    "                       'opt_optimizer': {'lr_method': 'Constant', 'lr': 1e-4}},\n",
    "          'Conv-LTSM': {'model': 'Conv-LTSM', 'run': True,\n",
    "                        'opt_model': {'latent_dim': 64},\n",
    "                        'opt_optimizer': {'lr_method': 'Constant'}},\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231ef4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the amount of precipitation\n",
    "df_prec = pd.DataFrame(columns = ['id', 'name', 'n_params', 'opt_model', 'opt_optimizer',\n",
    "                                  'train_pr_rmse', 'test_pr_rmse', \n",
    "                                  'train_xtrm_precision', 'test_xtrm_precision', \n",
    "                                  'train_xtrm_recall', 'test_xtrm_recall'])\n",
    "df_xtrm = pd.DataFrame(columns = ['id', 'name', 'n_params', 'opt_model', 'opt_optimizer',\n",
    "                                  'train_pr_rmse', 'test_pr_rmse', \n",
    "                                  'train_xtrm_precision', 'test_xtrm_precision', \n",
    "                                  'train_xtrm_recall', 'test_xtrm_recall', \n",
    "                                  'train_xtrm_roc_auc', 'test_xtrm_roc_auc'])\n",
    "\n",
    "train_for_prec = True\n",
    "train_for_xtrm = True\n",
    "\n",
    "if train_for_prec:\n",
    "        \n",
    "    for m_id in models:\n",
    "        # Clear session and set tf seed\n",
    "        keras.backend.clear_session()\n",
    "        tf.random.set_seed(42)\n",
    "        \n",
    "        if not models[m_id]['run']:\n",
    "            continue\n",
    "\n",
    "        # Extract model name and options\n",
    "        model = models[m_id]['model']\n",
    "        opt_model_i = models[m_id]['opt_model']\n",
    "        opt_optimizer_i = models[m_id]['opt_optimizer']\n",
    "        opt_model_new = opt_model.copy()\n",
    "        opt_model_new.update(opt_model_i)\n",
    "        opt_optimizer_new = opt_optimizer.copy()\n",
    "        opt_optimizer_new.update(opt_optimizer_i)\n",
    "        print(f'Running: {m_id} - {model} - {opt_model_i} - {opt_optimizer_i}')\n",
    "        df_prec = df_prec.append({'id': m_id, 'name': model, 'opt_model': opt_model_i, 'opt_optimizer': opt_optimizer_i}, ignore_index=True)\n",
    "\n",
    "        # Switch to precipitation values\n",
    "        dg_train.for_extremes(False)\n",
    "        dg_valid.for_extremes(False)\n",
    "        dg_test.for_extremes(False)\n",
    "        \n",
    "        optimizer = initiate_optimizer(**opt_optimizer_new)\n",
    "\n",
    "        # Create the model and compile\n",
    "        m = DeepFactory(model, i_shape, o_shape, for_extremes=False, **opt_model_new)\n",
    "        # Warning: When using regularizers, the loss function is the entire loss, ie (loss metrics) + (regularization term)!\n",
    "        # But the loss displayed as part of the metrics, is only the loss metric. The regularization term is not added there. -> can be different!!\n",
    "        m.compile(\n",
    "            loss='mse', \n",
    "            metrics=['mse'], \n",
    "            optimizer=optimizer\n",
    "        )\n",
    "        print(f'Number of parameters: {m.model.count_params()}')\n",
    "        df_prec.at[df_prec.index[-1], 'n_params'] = m.model.count_params()\n",
    "\n",
    "        # Train\n",
    "        hist = m.fit(dg_train, validation_data=dg_valid, verbose=0, **opt_training)\n",
    "        \n",
    "        # Predict and save scores\n",
    "        y_pred_train = m.predict(dg_train.X.to_numpy()).squeeze()\n",
    "        y_pred_test = m.predict(dg_test.X.to_numpy()).squeeze()\n",
    "        df_prec.at[df_prec.index[-1], 'train_pr_rmse'] = np.sqrt(np.square(np.subtract(dg_train.y.to_numpy().squeeze(), y_pred_train)).mean())\n",
    "        df_prec.at[df_prec.index[-1], 'test_pr_rmse'] =  np.sqrt(np.square(np.subtract(dg_test.y.to_numpy().squeeze(), y_pred_test)).mean())\n",
    "\n",
    "        # Plot training evolution\n",
    "        pd.DataFrame(hist.history).plot(figsize=(8, 5))\n",
    "        ymin = 0.8 * min([min(hist.history['loss']), min(hist.history['val_loss']), min(hist.history['mse'])])\n",
    "        ymax = min(max(max(hist.history['loss']), max(hist.history['val_loss'])), \n",
    "                   2 * max(sum(hist.history['loss'])/len(hist.history['loss']), sum(hist.history['val_loss'])/len(hist.history['val_loss'])))\n",
    "        plt.ylim(ymin, ymax)\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        # Analyze predictions\n",
    "        print('Plotting results of the training period.')\n",
    "        precision, recall = analyze_predictions(y_pred_train, dg_train, qq, pred_xtrm=False, show_plots=False)\n",
    "        df_prec.at[df_prec.index[-1], 'train_xtrm_precision'] = precision\n",
    "        df_prec.at[df_prec.index[-1], 'train_xtrm_recall'] = recall\n",
    "        plt.show()\n",
    "        \n",
    "        print('Plotting results of the testing period.')\n",
    "        precision, recall = analyze_predictions(y_pred_test, dg_test, qq, pred_xtrm=False)\n",
    "        df_prec.at[df_prec.index[-1], 'test_xtrm_precision'] = precision\n",
    "        df_prec.at[df_prec.index[-1], 'test_xtrm_recall'] = recall\n",
    "        plt.show()\n",
    "    \n",
    "        print(df_prec.iloc[-1])\n",
    "        \n",
    "        print(f\"\\n{'*' * 100}\\n\")\n",
    "        \n",
    "if train_for_xtrm:\n",
    "\n",
    "    for m_id in models:\n",
    "        # Clear session and set tf seed\n",
    "        keras.backend.clear_session()\n",
    "        tf.random.set_seed(42)\n",
    "\n",
    "        if not models[m_id]['run']:\n",
    "            continue\n",
    "        \n",
    "        # Extract model name and options\n",
    "        model = models[m_id]['model']\n",
    "        opt_model_i = models[m_id]['opt_model']\n",
    "        opt_optimizer_i = models[m_id]['opt_optimizer']\n",
    "        opt_model_new = opt_model.copy()\n",
    "        opt_model_new.update(opt_model_i)\n",
    "        opt_optimizer_new = opt_optimizer.copy()\n",
    "        opt_optimizer_new.update(opt_optimizer_i)\n",
    "        print(f'Running: {m_id} - {model} - {opt_model_i} - {opt_optimizer_i}')\n",
    "        df_xtrm = df_xtrm.append({'id': m_id, 'name': model, 'opt_model': opt_model_i, 'opt_optimizer': opt_optimizer_i}, ignore_index=True)\n",
    "        \n",
    "        # Switch to precipitation extremes\n",
    "        dg_train.for_extremes(True)\n",
    "        dg_valid.for_extremes(True)\n",
    "        dg_test.for_extremes(True)\n",
    "        \n",
    "        optimizer = initiate_optimizer(**opt_optimizer_new)\n",
    "\n",
    "        # Create the model and compile\n",
    "        m = DeepFactory(model, i_shape, o_shape, for_extremes=True, **opt_model_new)\n",
    "        m.compile(\n",
    "            loss=xtrm_loss,\n",
    "            optimizer=optimizer\n",
    "        )\n",
    "        print(f'Number of parameters: {m.model.count_params()}')\n",
    "        df_xtrm.at[df_xtrm.index[-1], 'n_params'] = m.model.count_params()\n",
    "\n",
    "        # Train\n",
    "        hist = m.fit(dg_train, validation_data=dg_valid, verbose=1, **opt_training)\n",
    "        \n",
    "        # Assess and save scores\n",
    "        y_pred_train = m.predict(dg_train.X.to_numpy()).squeeze()\n",
    "        y_pred_test = m.predict(dg_test.X.to_numpy()).squeeze()\n",
    "    \n",
    "        # Plot training evolution\n",
    "        pd.DataFrame(hist.history).plot(figsize=(8, 5))\n",
    "        ymin = 0.8 * min([min(hist.history['loss']), min(hist.history['val_loss'])])\n",
    "        ymax = min(max(max(hist.history['loss']), max(hist.history['val_loss'])), \n",
    "                   2 * max(sum(hist.history['loss'])/len(hist.history['loss']), sum(hist.history['val_loss'])/len(hist.history['val_loss'])))\n",
    "        plt.ylim(ymin, ymax)\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        # Analyze predictions\n",
    "        print('Plotting results of the training period.')\n",
    "        precision, recall, roc_auc = analyze_predictions(y_pred_train, dg_train, qq, pred_xtrm=True, show_plots=False)\n",
    "        df_xtrm.at[df_xtrm.index[-1], 'train_xtrm_precision'] = precision\n",
    "        df_xtrm.at[df_xtrm.index[-1], 'train_xtrm_recall'] = recall\n",
    "        df_xtrm.at[df_xtrm.index[-1], 'train_xtrm_roc_auc'] = roc_auc\n",
    "        plt.show()\n",
    "        \n",
    "        print('Plotting results of the testing period.')\n",
    "        precision, recall, roc_auc = analyze_predictions(y_pred_test, dg_test, qq, pred_xtrm=True)\n",
    "        df_xtrm.at[df_xtrm.index[-1], 'test_xtrm_precision'] = precision\n",
    "        df_xtrm.at[df_xtrm.index[-1], 'test_xtrm_recall'] = recall\n",
    "        df_xtrm.at[df_xtrm.index[-1], 'test_xtrm_roc_auc'] = roc_auc\n",
    "        plt.show()\n",
    "\n",
    "        print(df_xtrm.iloc[-1])\n",
    "        \n",
    "        print(f\"\\n{'*' * 100}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a495bdc0-fc28-4433-9b28-4e9627a67d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5df59f7-5926-46ac-be5d-f7ca5b71d493",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xtrm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd09fcf-b4c1-4ec1-8936-4c82e0275f00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bee8a8-7f4b-4ef6-8068-9df90d0c434a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
